# LLM í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ í•™ìŠµì— í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ

### 1ï¸âƒ£ **ì†ì‹¤ í•¨ìˆ˜ (Loss Function)**
```python
loss = CrossEntropyLoss(logits, targets)
```
- **ì—­í• **: ëª¨ë¸ ì˜ˆì¸¡ê³¼ ì •ë‹µ ì‚¬ì´ì˜ ì°¨ì´ ì¸¡ì •
- **Cross Entropy Loss**: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ì‚¬ìš©
- **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ**: ëª¨ë¸ì´ ì •ë‹µì„ ì˜ ì˜ˆì¸¡í•¨

### 2ï¸âƒ£ **ì˜µí‹°ë§ˆì´ì € (Optimizer)**
```python
optimizer = AdamW(
    model.parameters(),
    lr=3e-4,              # í•™ìŠµë¥ 
    weight_decay=0.1,     # L2 ì •ê·œí™”
    betas=(0.9, 0.95)     # ëª¨ë©˜í…€ íŒŒë¼ë¯¸í„°
)
```
- **ì—­í• **: ì†ì‹¤ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
- **AdamW**: Adam + Weight Decay (LLMì—ì„œ í‘œì¤€)
- **í•™ìŠµë¥  (Learning Rate)**: ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°

### 3ï¸âƒ£ **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Learning Rate Scheduler)**
```
Warmup (0 â†’ max_lr) â†’ Cosine Decay (max_lr â†’ min_lr)
```
- **Warmup**: ì´ˆë°˜ì— í•™ìŠµë¥ ì„ ì²œì²œíˆ ì¦ê°€ â†’ í•™ìŠµ ì•ˆì •í™”
- **Cosine Decay**: í•™ìŠµë¥ ì„ ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ê°ì†Œ â†’ ë¶€ë“œëŸ¬ìš´ ìˆ˜ë ´

### 4ï¸âƒ£ **ë°ì´í„°ë¡œë” (DataLoader)**
```python
DataLoader(dataset, batch_size=8, shuffle=True)
```
- **ì—­í• **: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì œê³µ
- **Shuffle**: í•™ìŠµ ë°ì´í„° ìˆœì„œ ì„ìŒ â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- **Batch Size**: í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ìƒ˜í”Œ ìˆ˜

### 5ï¸âƒ£ **í•™ìŠµ ë£¨í”„ (Training Loop)**
```python
for epoch in range(num_epochs):
    for batch in train_loader:
        # 1. ìˆœì „íŒŒ
        logits = model(inputs)

        # 2. ì†ì‹¤ ê³„ì‚°
        loss = compute_loss(logits, targets)

        # 3. ì—­ì „íŒŒ
        loss.backward()

        # 4. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        optimizer.step()
        optimizer.zero_grad()
```

### 6ï¸âƒ£ **ê²€ì¦ (Validation)**
```python
model.eval()
with torch.no_grad():
    val_loss = evaluate(model, val_loader)
```
- **ì—­í• **: ê³¼ì í•© í™•ì¸
- **ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€**í•˜ë©´ â†’ ê³¼ì í•© ë°œìƒ

### 7ï¸âƒ£ **ì²´í¬í¬ì¸íŠ¸ (Checkpointing)**
```python
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'best_val_loss': best_val_loss
}, 'checkpoint.pt')
```
- **ì—­í• **: í•™ìŠµ ì¤‘ê°„ ê²°ê³¼ ì €ì¥
- **Best Model**: ê²€ì¦ ì†ì‹¤ì´ ê°€ì¥ ë‚®ì€ ëª¨ë¸ ì €ì¥

### 8ï¸âƒ£ **í‰ê°€ ì§€í‘œ (Metrics)**

#### **Perplexity (í˜¼ë€ë„)**
```python
perplexity = exp(loss)
```
- **ì˜ë¯¸**: ëª¨ë¸ì´ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•  ë•Œì˜ ë¶ˆí™•ì‹¤ì„±
- **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ**
- **ì˜ˆì‹œ**:
  - Perplexity = 10 â†’ í‰ê·  10ê°œ í›„ë³´ ì¤‘ ì„ íƒ
  - Perplexity = 100 â†’ í‰ê·  100ê°œ í›„ë³´ ì¤‘ ì„ íƒ

#### **Accuracy (ì •í™•ë„)**
```python
accuracy = (predictions == targets).mean()
```
- **ì˜ë¯¸**: ë‹¤ìŒ í† í°ì„ ì •í™•íˆ ë§ì¶˜ ë¹„ìœ¨
- **ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ** (0~1)

---

## ğŸ”§ í•™ìŠµ ì‹¤í–‰ ë°©ë²•

### **ê¸°ë³¸ í•™ìŠµ**
```bash
python training_pipeline.py
```

### **í•™ìŠµ ì„¤ì • ìˆ˜ì •**
`training_pipeline.py`ì˜ `config` ë”•ì…”ë„ˆë¦¬ ìˆ˜ì •:

```python
config = {
    # ëª¨ë¸ í¬ê¸°
    'd_embed': 256,        # ì„ë² ë”© ì°¨ì› (í´ìˆ˜ë¡ ì„±ëŠ¥â†‘, ëŠë¦¼â†‘)
    'num_heads': 4,        # í—¤ë“œ ê°œìˆ˜
    'num_layers': 4,       # ë ˆì´ì–´ ê°œìˆ˜ (ê¹Šì„ìˆ˜ë¡ ì„±ëŠ¥â†‘)

    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    'batch_size': 8,       # ë°°ì¹˜ í¬ê¸° (í´ìˆ˜ë¡ ì•ˆì •ì , ë©”ëª¨ë¦¬â†‘)
    'num_epochs': 10,      # ì—í¬í¬ ìˆ˜
    'learning_rate': 3e-4, # í•™ìŠµë¥  (ê°€ì¥ ì¤‘ìš”!)

    # ì •ê·œí™”
    'dropout': 0.1,        # ë“œë¡­ì•„ì›ƒ (ê³¼ì í•© ë°©ì§€)
    'weight_decay': 0.1,   # L2 ì •ê·œí™”
}
```

---

## ğŸ“Š í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§

### **í•™ìŠµ ì¤‘ ì¶œë ¥ ì˜ˆì‹œ**
```
Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [02:30<00:00,  1.00it/s, loss=5.2341, acc=0.1523, lr=1.5e-04]

Epoch 1/10
  Train Loss: 5.2341, PPL: 187.23, Acc: 0.1523
  Val   Loss: 5.1234, PPL: 168.45, Acc: 0.1678
  LR: 1.5e-04
  âœ… ìµœê³  ëª¨ë¸ ì €ì¥! (Val Loss: 5.1234)
```

### **ì£¼ìš” ì§€í‘œ í•´ì„**

| ì§€í‘œ | ì˜ë¯¸ | ëª©í‘œ |
|------|------|------|
| **Train Loss** | í•™ìŠµ ì†ì‹¤ | ê°ì†Œ |
| **Val Loss** | ê²€ì¦ ì†ì‹¤ | ê°ì†Œ (Trainê³¼ ë¹„ìŠ·) |
| **PPL (Perplexity)** | í˜¼ë€ë„ | ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **Acc (Accuracy)** | ì •í™•ë„ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **LR** | í˜„ì¬ í•™ìŠµë¥  | Warmup í›„ ê°ì†Œ |

### **ê³¼ì í•© ì§•í›„**
```
Epoch 5:
  Train Loss: 2.1  â† ê³„ì† ê°ì†Œ
  Val Loss: 3.5    â† ì¦ê°€í•˜ê¸° ì‹œì‘!
```
â†’ **í•´ê²°ì±…**: Dropout ì¦ê°€, Weight Decay ì¦ê°€, ì¡°ê¸° ì¢…ë£Œ

---

## ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### **1. í•™ìŠµë¥  (Learning Rate)** â­â­â­â­â­
- **ê°€ì¥ ì¤‘ìš”!**
- **ë„ˆë¬´ í¬ë©´**: ë°œì‚° (loss = NaN)
- **ë„ˆë¬´ ì‘ìœ¼ë©´**: í•™ìŠµì´ ëŠë¦¼
- **ê¶Œì¥ ë²”ìœ„**: 1e-4 ~ 1e-3
- **ì°¾ëŠ” ë°©ë²•**: Learning Rate Finder ì‚¬ìš©

### **2. ë°°ì¹˜ í¬ê¸° (Batch Size)** â­â­â­â­
- **í° ë°°ì¹˜**: ì•ˆì •ì , ë¹ ë¦„, ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
- **ì‘ì€ ë°°ì¹˜**: ë¶ˆì•ˆì •, ëŠë¦¼, ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©
- **ê¶Œì¥**: GPU ë©”ëª¨ë¦¬ê°€ í—ˆìš©í•˜ëŠ” ìµœëŒ€ í¬ê¸°
- **ì¼ë°˜ì **: 8, 16, 32, 64

### **3. ëª¨ë¸ í¬ê¸°** â­â­â­â­
- **d_embed**: 128(ì‘ìŒ) â†’ 256(ì¤‘ê°„) â†’ 512(í¼) â†’ 768(ë§¤ìš° í¼)
- **num_layers**: 4(ì‘ìŒ) â†’ 6(ì¤‘ê°„) â†’ 12(í¼)
- **ì›ì¹™**: ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ í° ëª¨ë¸

### **4. Dropout** â­â­â­
- **ê³¼ì í•© ë°©ì§€**
- **ì¼ë°˜ì **: 0.1
- **ë°ì´í„° ì ì„ ë•Œ**: 0.2~0.3
- **ë°ì´í„° ë§ì„ ë•Œ**: 0.0~0.1

### **5. Weight Decay** â­â­â­
- **L2 ì •ê·œí™”**
- **ì¼ë°˜ì **: 0.1
- **ê³¼ì í•© ì‹¬í•  ë•Œ**: 0.2~0.5

---

## ğŸ’¡ í•™ìŠµ íŒ

### **1. ì‘ê²Œ ì‹œì‘í•˜ê¸°**
```python
# ë¹ ë¥¸ ì‹¤í—˜ìš© ì„¤ì •
config = {
    'd_embed': 128,
    'num_layers': 2,
    'batch_size': 4,
    'num_epochs': 2
}
```
â†’ ì½”ë“œê°€ ì‘ë™í•˜ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸

### **2. í•™ìŠµë¥  ì°¾ê¸°**
```python
# ì—¬ëŸ¬ í•™ìŠµë¥  ì‹œë„
learning_rates = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]
for lr in learning_rates:
    # ëª‡ ì—í¬í¬ë§Œ í•™ìŠµ
    # ì†ì‹¤ì´ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” lr ì„ íƒ
```

### **3. ê³¼ì í•© ëª¨ë‹ˆí„°ë§**
```python
if val_loss > best_val_loss + 0.5:
    print("âš ï¸  ê³¼ì í•© ë°œìƒ!")
    # Dropout ì¦ê°€ ë˜ëŠ” ì¡°ê¸° ì¢…ë£Œ
```

### **4. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘**
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```
â†’ ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€ (í•„ìˆ˜!)

### **5. Mixed Precision Training**
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    logits = model(inputs)
    loss = compute_loss(logits, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```
â†’ GPU ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ

---

## ğŸ“ˆ í•™ìŠµ ì˜ˆìƒ ê²°ê³¼

### **ì‘ì€ ë°ì´í„°ì…‹ (20KB, the-verdict.txt)**
```
ì´ˆê¸° (Epoch 1):
  PPL: ~180, Acc: ~15%

ì¤‘ê°„ (Epoch 5):
  PPL: ~80, Acc: ~30%

ìµœì¢… (Epoch 10):
  PPL: ~50, Acc: ~40%
```

### **í° ë°ì´í„°ì…‹ (ìˆ˜ GB)**
```
ì´ˆê¸°: PPL: ~100
ì¤‘ê°„: PPL: ~30
ìµœì¢…: PPL: ~10-20 (GPT-2 ìˆ˜ì¤€)
```

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

í•™ìŠµ í›„ í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤:

1. **í…ìŠ¤íŠ¸ ìƒì„±**
```python
from training_pipeline import Trainer
trainer.load_checkpoint('checkpoints/best_model.pt')
generated = generate_text(model, "Once upon a time", max_tokens=100)
```

2. **Fine-tuning**
- íŠ¹ì • ë„ë©”ì¸ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ
- ì‘ì€ í•™ìŠµë¥  (1e-5) ì‚¬ìš©

3. **ëª¨ë¸ í‰ê°€**
- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸
- Perplexityë¡œ ì •ëŸ‰ í‰ê°€

4. **ë°°í¬**
- ONNX ë³€í™˜
- ì–‘ìí™” (Quantization)
- API ì„œë²„ êµ¬ì¶•

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: "Attention Is All You Need" (Transformer ì›ë³¸)
- **GPT-2**: "Language Models are Unsupervised Multitask Learners"
- **GPT-3**: "Language Models are Few-Shot Learners"
- **ìµœì í™”**: "Decoupled Weight Decay Regularization" (AdamW)

---

## â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ê°€ ë‚˜ìš”
```
A: batch_sizeë¥¼ ì¤„ì´ê±°ë‚˜ max_seq_lenì„ ì¤„ì´ì„¸ìš”
   ë˜ëŠ” gradient_accumulation ì‚¬ìš©
```

### Q: ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•Šì•„ìš”
```
A: 1. í•™ìŠµë¥ ì„ ë‚®ì¶”ì„¸ìš” (1e-4 â†’ 1e-5)
   2. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ í™•ì¸
   3. ë°ì´í„° í’ˆì§ˆ í™•ì¸
```

### Q: ì–¼ë§ˆë‚˜ í•™ìŠµí•´ì•¼ í•˜ë‚˜ìš”?
```
A: ê²€ì¦ ì†ì‹¤ì´ ë” ì´ìƒ ê°ì†Œí•˜ì§€ ì•Šì„ ë•Œê¹Œì§€
   ë³´í†µ 10~100 ì—í¬í¬
```

### Q: ëª¨ë¸ì´ ê°™ì€ ë‹¨ì–´ë§Œ ë°˜ë³µí•´ìš”
```
A: Temperatureë¥¼ ë†’ì´ê±°ë‚˜ Top-k/Top-p ìƒ˜í”Œë§ ì‚¬ìš©
   ë˜ëŠ” ë” ë§ì€ ë°ì´í„°ë¡œ ë” ì˜¤ë˜ í•™ìŠµ
```
