"""
ì¸µ ì •ê·œí™” (Layer Normalization)

LLMì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ:
1. Layer Normalization: ê° ì¸µì˜ ì¶œë ¥ì„ ì •ê·œí™”í•˜ì—¬ í•™ìŠµ ì•ˆì •í™”
2. Residual Connection (Skip Connection): ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°
3. Feed Forward Network: ì¶”ê°€ì ì¸ ë¹„ì„ í˜• ë³€í™˜

Transformer ë¸”ë¡ êµ¬ì¡°:
Input â†’ LayerNorm â†’ Multi-Head Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm â†’ Output
"""

import torch
import torch.nn as nn
import tiktoken
import sys


class LayerNorm(nn.Module):
    """
    ì¸µ ì •ê·œí™” (Layer Normalization)

    ë°°ì¹˜ ì •ê·œí™”(Batch Norm)ì™€ ë‹¬ë¦¬ ê° ìƒ˜í”Œì„ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”
    â†’ ë°°ì¹˜ í¬ê¸°ì— ì˜í–¥ì„ ë°›ì§€ ì•Šì•„ NLPì—ì„œ ë” íš¨ê³¼ì 

    ì‘ë™ ì›ë¦¬:
    1. ê° ìƒ˜í”Œì˜ í‰ê· (mean)ê³¼ ë¶„ì‚°(variance) ê³„ì‚°
    2. ì •ê·œí™”: (x - mean) / sqrt(variance + eps)
    3. ìŠ¤ì¼€ì¼(Î³)ê³¼ ì‹œí”„íŠ¸(Î²) í•™ìŠµ íŒŒë¼ë¯¸í„° ì ìš©
    """

    def __init__(self, d_embed, eps=1e-5):
        """
        Args:
            d_embed: ì„ë² ë”© ì°¨ì›
            eps: ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’ (0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€)
        """
        super().__init__()
        self.eps = eps

        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
        self.scale = nn.Parameter(torch.ones(d_embed))   # Î³ (ê°ë§ˆ)
        self.shift = nn.Parameter(torch.zeros(d_embed))  # Î² (ë² íƒ€)

    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, d_embed]

        Returns:
            normalized: [batch_size, seq_len, d_embed]
        """
        # ë§ˆì§€ë§‰ ì°¨ì›(d_embed)ì— ëŒ€í•´ í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°
        mean = x.mean(dim=-1, keepdim=True)  # [batch_size, seq_len, 1]
        var = x.var(dim=-1, keepdim=True, unbiased=False)  # [batch_size, seq_len, 1]

        # ì •ê·œí™”
        normalized = (x - mean) / torch.sqrt(var + self.eps)

        # ìŠ¤ì¼€ì¼ê³¼ ì‹œí”„íŠ¸ ì ìš©
        output = self.scale * normalized + self.shift

        return output


class FeedForward(nn.Module):
    """
    í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (Feed Forward Network)

    êµ¬ì¡°: Linear â†’ GELU â†’ Linear
    - ì¤‘ê°„ ì°¨ì›ì„ 4ë°°ë¡œ í™•ì¥í–ˆë‹¤ê°€ ë‹¤ì‹œ ì¶•ì†Œ
    - GELU: ë¶€ë“œëŸ¬ìš´ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ (ReLUë³´ë‹¤ ì„±ëŠ¥ ì¢‹ìŒ)
    """

    def __init__(self, d_embed, d_ff=None, dropout=0.1):
        """
        Args:
            d_embed: ì„ë² ë”© ì°¨ì›
            d_ff: í”¼ë“œí¬ì›Œë“œ ì¤‘ê°„ ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ d_embedì˜ 4ë°°)
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        """
        super().__init__()

        if d_ff is None:
            d_ff = 4 * d_embed  # ê¸°ë³¸ê°’: 4ë°° í™•ì¥

        self.fc1 = nn.Linear(d_embed, d_ff)
        self.gelu = nn.GELU()
        self.fc2 = nn.Linear(d_ff, d_embed)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, d_embed]

        Returns:
            output: [batch_size, seq_len, d_embed]
        """
        x = self.fc1(x)      # [batch_size, seq_len, d_ff]
        x = self.gelu(x)     # ë¹„ì„ í˜• í™œì„±í™”
        x = self.fc2(x)      # [batch_size, seq_len, d_embed]
        x = self.dropout(x)
        return x


class MultiHeadAttention(nn.Module):
    """ë©€í‹°-í—¤ë“œ ì–´í…ì…˜"""

    def __init__(self, d_embed, num_heads, dropout=0.1):
        super().__init__()
        assert d_embed % num_heads == 0

        self.d_embed = d_embed
        self.num_heads = num_heads
        self.head_dim = d_embed // num_heads

        self.W_query = nn.Linear(d_embed, d_embed)
        self.W_key = nn.Linear(d_embed, d_embed)
        self.W_value = nn.Linear(d_embed, d_embed)
        self.out_proj = nn.Linear(d_embed, d_embed)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        batch_size, seq_len, d_embed = x.shape

        # Query, Key, Value ê³„ì‚°
        queries = self.W_query(x)
        keys = self.W_key(x)
        values = self.W_value(x)

        # ë©€í‹°-í—¤ë“œë¡œ ë¶„í• 
        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled Dot-Product Attention
        attn_scores = queries @ keys.transpose(2, 3) / (self.head_dim ** 0.5)

        # Causal mask (ë””ë°”ì´ìŠ¤ ì¼ì¹˜)
        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)
        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))

        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Context ê³„ì‚°
        context = attn_weights @ values
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_embed)

        # ì¶œë ¥ íˆ¬ì˜
        output = self.out_proj(context)
        return output


class TransformerBlock(nn.Module):
    """
    Transformer ë¸”ë¡ (GPT ìŠ¤íƒ€ì¼)

    êµ¬ì¡°:
    1. LayerNorm â†’ Multi-Head Attention â†’ Residual Connection
    2. LayerNorm â†’ Feed Forward Network â†’ Residual Connection

    Pre-LN (Pre-Layer Normalization):
    - LayerNormì„ ì„œë¸Œë ˆì´ì–´ ì•ì— ë°°ì¹˜
    - í•™ìŠµ ì•ˆì •ì„±ì´ ë” ì¢‹ìŒ (GPT-3, GPT-4ì—ì„œ ì‚¬ìš©)
    """

    def __init__(self, d_embed, num_heads, dropout=0.1, d_ff=None):
        """
        Args:
            d_embed: ì„ë² ë”© ì°¨ì›
            num_heads: í—¤ë“œ ê°œìˆ˜
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            d_ff: í”¼ë“œí¬ì›Œë“œ ì¤‘ê°„ ì°¨ì›
        """
        super().__init__()

        # Layer Normalization
        self.ln1 = LayerNorm(d_embed)
        self.ln2 = LayerNorm(d_embed)

        # Multi-Head Attention
        self.attn = MultiHeadAttention(d_embed, num_heads, dropout)

        # Feed Forward Network
        self.ffn = FeedForward(d_embed, d_ff, dropout)

        # Dropout (Residual Connectionì— ì ìš©)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, d_embed]

        Returns:
            output: [batch_size, seq_len, d_embed]
        """
        # 1. Multi-Head Attention with Residual Connection
        # Pre-LN: LayerNorm â†’ Attention
        attn_output = self.attn(self.ln1(x))
        x = x + self.dropout(attn_output)  # Residual Connection

        # 2. Feed Forward Network with Residual Connection
        # Pre-LN: LayerNorm â†’ FFN
        ffn_output = self.ffn(self.ln2(x))
        x = x + self.dropout(ffn_output)  # Residual Connection

        return x


class SimpleLLM(nn.Module):
    """
    ê°„ë‹¨í•œ LLM ëª¨ë¸

    êµ¬ì¡°:
    Token Embedding â†’ Positional Embedding â†’ N Ã— Transformer Blocks â†’ LayerNorm â†’ Output
    """

    def __init__(self, vocab_size, d_embed=256, num_heads=4, num_layers=4,
                 max_seq_len=512, dropout=0.1):
        """
        Args:
            vocab_size: ì–´íœ˜ í¬ê¸°
            d_embed: ì„ë² ë”© ì°¨ì›
            num_heads: í—¤ë“œ ê°œìˆ˜
            num_layers: Transformer ë¸”ë¡ ê°œìˆ˜
            max_seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        """
        super().__init__()

        self.d_embed = d_embed

        # Token Embedding
        self.token_embedding = nn.Embedding(vocab_size, d_embed)

        # Positional Embedding (í•™ìŠµ ê°€ëŠ¥)
        self.pos_embedding = nn.Embedding(max_seq_len, d_embed)

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # Transformer Blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_embed, num_heads, dropout)
            for _ in range(num_layers)
        ])

        # ìµœì¢… Layer Normalization
        self.ln_final = LayerNorm(d_embed)

        # Output projection (í† í° ì˜ˆì¸¡)
        self.lm_head = nn.Linear(d_embed, vocab_size, bias=False)

        print(f"âœ… SimpleLLM ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì–´íœ˜ í¬ê¸°: {vocab_size:,}")
        print(f"   - ì„ë² ë”© ì°¨ì›: {d_embed}")
        print(f"   - í—¤ë“œ ê°œìˆ˜: {num_heads}")
        print(f"   - ë ˆì´ì–´ ê°œìˆ˜: {num_layers}")
        print(f"   - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_seq_len}")
        print(f"   - ë“œë¡­ì•„ì›ƒ: {dropout}")

        # íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°
        num_params = sum(p.numel() for p in self.parameters())
        print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {num_params:,}\n")

    def forward(self, token_ids):
        """
        Args:
            token_ids: [batch_size, seq_len]

        Returns:
            logits: [batch_size, seq_len, vocab_size]
        """
        batch_size, seq_len = token_ids.shape

        # Token Embedding
        token_embeds = self.token_embedding(token_ids)  # [batch_size, seq_len, d_embed]

        # Positional Embedding
        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)  # [1, seq_len]
        pos_embeds = self.pos_embedding(positions)  # [1, seq_len, d_embed]

        # ì„ë² ë”© ê²°í•©
        x = token_embeds + pos_embeds
        x = self.dropout(x)

        # Transformer Blocks
        for block in self.blocks:
            x = block(x)

        # ìµœì¢… LayerNorm
        x = self.ln_final(x)

        # ì¶œë ¥ íˆ¬ì˜ (ë¡œì§“)
        logits = self.lm_head(x)  # [batch_size, seq_len, vocab_size]

        return logits


def test_layer_normalization():
    """ì¸µ ì •ê·œí™” í…ŒìŠ¤íŠ¸"""
    print("="*80)
    print("ğŸ§ª ì¸µ ì •ê·œí™” (Layer Normalization) í…ŒìŠ¤íŠ¸")
    print("="*80)
    print()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 2
    seq_len = 4
    d_embed = 8

    x = torch.randn(batch_size, seq_len, d_embed)
    print(f"ì…ë ¥ í¬ê¸°: {x.shape}")
    print(f"ì…ë ¥ ë°ì´í„° (ì²« ë²ˆì§¸ ìƒ˜í”Œ, ì²« ë²ˆì§¸ í† í°):\n{x[0, 0]}\n")

    # Layer Normalization ì ìš©
    ln = LayerNorm(d_embed)
    normalized = ln(x)

    print(f"ì •ê·œí™” í›„ í¬ê¸°: {normalized.shape}")
    print(f"ì •ê·œí™” í›„ ë°ì´í„° (ì²« ë²ˆì§¸ ìƒ˜í”Œ, ì²« ë²ˆì§¸ í† í°):\n{normalized[0, 0]}\n")

    # í†µê³„ í™•ì¸
    mean = normalized[0, 0].mean()
    std = normalized[0, 0].std(unbiased=False)
    print(f"ì •ê·œí™” í›„ í‰ê· : {mean:.6f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)")
    print(f"ì •ê·œí™” í›„ í‘œì¤€í¸ì°¨: {std:.6f} (1ì— ê°€ê¹Œì›Œì•¼ í•¨)")
    print()


def test_transformer_block():
    """Transformer ë¸”ë¡ í…ŒìŠ¤íŠ¸"""
    print("="*80)
    print("ğŸ§ª Transformer ë¸”ë¡ í…ŒìŠ¤íŠ¸")
    print("="*80)
    print()

    batch_size = 2
    seq_len = 6
    d_embed = 128
    num_heads = 4

    x = torch.randn(batch_size, seq_len, d_embed)
    print(f"ì…ë ¥ í¬ê¸°: {x.shape}")

    # Transformer Block
    block = TransformerBlock(d_embed, num_heads, dropout=0.1)
    output = block(x)

    print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")
    print(f"âœ… Transformer ë¸”ë¡ ì‘ë™ í™•ì¸\n")

    # íŒŒë¼ë¯¸í„° ê°œìˆ˜
    num_params = sum(p.numel() for p in block.parameters())
    print(f"Transformer ë¸”ë¡ íŒŒë¼ë¯¸í„° ìˆ˜: {num_params:,}\n")


def test_simple_llm():
    """SimpleLLM í…ŒìŠ¤íŠ¸"""
    print("="*80)
    print("ğŸ¤– SimpleLLM ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*80)
    print()

    # í† í¬ë‚˜ì´ì €
    tokenizer = tiktoken.get_encoding("gpt2")
    vocab_size = tokenizer.n_vocab

    # ëª¨ë¸ ìƒì„±
    torch.manual_seed(42)
    model = SimpleLLM(
        vocab_size=vocab_size,
        d_embed=256,
        num_heads=4,
        num_layers=4,
        max_seq_len=512,
        dropout=0.1
    )

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥
    text = "Hello, how are you today?"
    token_ids = tokenizer.encode(text)
    token_tensor = torch.tensor(token_ids).unsqueeze(0)  # [1, seq_len]

    print(f"ğŸ“ ì…ë ¥ ë¬¸ì¥: \"{text}\"")
    print(f"í† í° ID: {token_ids}")
    print(f"ì…ë ¥ í¬ê¸°: {token_tensor.shape}\n")

    # ìˆœì „íŒŒ
    model.eval()
    with torch.no_grad():
        logits = model(token_tensor)

    print(f"ì¶œë ¥ ë¡œì§“ í¬ê¸°: {logits.shape}")
    print(f"  - Batch size: {logits.shape[0]}")
    print(f"  - Sequence length: {logits.shape[1]}")
    print(f"  - Vocab size: {logits.shape[2]}\n")

    # ë‹¤ìŒ í† í° ì˜ˆì¸¡
    last_token_logits = logits[0, -1, :]  # ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“
    probs = torch.softmax(last_token_logits, dim=-1)
    top5_probs, top5_indices = torch.topk(probs, 5)

    print("ğŸ¯ ë‹¤ìŒ í† í° ì˜ˆì¸¡ (Top 5):")
    for i, (prob, idx) in enumerate(zip(top5_probs, top5_indices), 1):
        token = tokenizer.decode([idx.item()])
        print(f"  {i}. '{token}' - í™•ë¥ : {prob.item():.4f} ({prob.item()*100:.2f}%)")


def compare_normalizations():
    """Batch Norm vs Layer Norm ë¹„êµ"""
    print("\n" + "="*80)
    print("ğŸ“Š Batch Normalization vs Layer Normalization ë¹„êµ")
    print("="*80)
    print()

    batch_size = 3
    seq_len = 4
    d_embed = 6

    x = torch.randn(batch_size, seq_len, d_embed)

    print(f"ì…ë ¥ í¬ê¸°: {x.shape}")
    print(f"ì…ë ¥ ë°ì´í„°:\n{x[0, 0]}\n")

    # Layer Normalization
    ln = LayerNorm(d_embed)
    ln_output = ln(x)

    print("Layer Normalization:")
    print("  - ê° ìƒ˜í”Œì„ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”")
    print("  - ë°°ì¹˜ í¬ê¸°ì— ì˜í–¥ ì—†ìŒ")
    print(f"  - ì¶œë ¥: {ln_output[0, 0]}")
    print(f"  - í‰ê· : {ln_output[0, 0].mean():.6f}, í‘œì¤€í¸ì°¨: {ln_output[0, 0].std(unbiased=False):.6f}\n")

    # PyTorchì˜ Layer Normê³¼ ë¹„êµ
    pytorch_ln = nn.LayerNorm(d_embed)
    pytorch_ln.weight.data = ln.scale.data
    pytorch_ln.bias.data = ln.shift.data
    pytorch_output = pytorch_ln(x)

    diff = (ln_output - pytorch_output).abs().max()
    print(f"âœ… PyTorch LayerNormê³¼ì˜ ì°¨ì´: {diff:.8f} (ê±°ì˜ ë™ì¼)\n")


if __name__ == "__main__":
    # 1. Layer Normalization í…ŒìŠ¤íŠ¸
    test_layer_normalization()

    # 2. Transformer Block í…ŒìŠ¤íŠ¸
    test_transformer_block()

    # 3. SimpleLLM ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    test_simple_llm()

    # 4. Normalization ë¹„êµ
    compare_normalizations()

    print("="*80)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)
