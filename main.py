"""
SimpleLLM - ë©”ì¸ ì‹¤í–‰ íŒŒì¼

ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ í•™ìŠµëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì´ í†µí•©ë˜ì–´ í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ì§ˆë¬¸ë„ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import tiktoken
import os
import sys
import re
from datetime import datetime

from layer_normalization import SimpleLLM


class LLMChatbot:
    """í•™ìŠµëœ LLMì„ ì‚¬ìš©í•œ ì±—ë´‡"""

    def __init__(self, checkpoint_path='checkpoints/best_model.pt'):
        """
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        """
        print("="*70)
        print("ğŸ¤– SimpleLLM ì±—ë´‡ ì´ˆê¸°í™” ì¤‘...")
        print("="*70)
        print()

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print("âœ… GPU ì‚¬ìš© (CUDA)")
        elif torch.backends.mps.is_available():
            self.device = torch.device('mps')
            print("âœ… GPU ì‚¬ìš© (Apple Silicon)")
        else:
            self.device = torch.device('cpu')
            print("âš ï¸  CPU ì‚¬ìš©")

        # í† í¬ë‚˜ì´ì €
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.vocab_size = self.tokenizer.n_vocab

        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        if not os.path.exists(checkpoint_path):
            print(f"\nâŒ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            print("ë¨¼ì € í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”: python training_pipeline.py")
            self.model = None
            self.trained = False
            return

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        print(f"\nğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        # ëª¨ë¸ ì„¤ì • ì¶”ì¶œ
        config = checkpoint.get('config', {})

        # ëª¨ë¸ ìƒì„±
        self.model = SimpleLLM(
            vocab_size=self.vocab_size,
            d_embed=config.get('d_embed', 256),
            num_heads=config.get('num_heads', 4),
            num_layers=config.get('num_layers', 4),
            max_seq_len=config.get('max_seq_len', 256),
            dropout=0.0  # ì¶”ë¡  ì‹œì—ëŠ” dropout ë¹„í™œì„±í™”
        ).to(self.device)

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        # í•™ìŠµ ì •ë³´
        epoch = checkpoint.get('epoch', 0)
        val_loss = checkpoint.get('best_val_loss', 0)

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"   - í•™ìŠµ ì—í¬í¬: {epoch + 1}")
        print(f"   - ìµœê³  ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
        print()

        self.trained = True

        # ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥
        self.web_search_enabled = True
        self.search_cache = {}

    def is_question(self, text):
        """ì…ë ¥ì´ ì§ˆë¬¸ì¸ì§€ íŒë‹¨"""
        question_patterns = [
            r'\?$',
            r'^(what|where|when|who|why|how|which|is|are|do|does|can|could|would|should)',
            r'(ë¬´ì—‡|ì–´ë””|ì–¸ì œ|ëˆ„êµ¬|ì™œ|ì–´ë–»ê²Œ|ë¬´ìŠ¨|ì–´ëŠ)',
            r'(ì¸ê°€ìš”|ì¸ì§€|ì…ë‹ˆê¹Œ|ì¸ê°€|í• ê¹Œ|ê¹Œìš”)$'
        ]

        text_lower = text.lower().strip()
        for pattern in question_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return True
        return False

    def is_response_valid(self, original_prompt, response):
        """ìƒì„±ëœ ì‘ë‹µì´ ìœ íš¨í•œì§€ íŒë‹¨"""
        if not response or response == original_prompt:
            return False

        response_only = response[len(original_prompt):].strip()
        if len(response_only) < 10:
            return False

        words = response_only.split()
        if len(words) > 5:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.3:
                return False

        return True

    def search_web(self, query):
        """ì›¹ì—ì„œ ì •ë³´ ê²€ìƒ‰"""
        if query in self.search_cache:
            print("  ğŸ“¦ ìºì‹œì—ì„œ ê°€ì ¸ì˜´")
            return self.search_cache[query]

        print(f"  ğŸ” ì›¹ ê²€ìƒ‰: '{query}'...")

        try:
            # WebSearch ì‚¬ìš© ì‹œë„
            import importlib
            if importlib.util.find_spec("anthropic") is not None:
                # WebSearch ë„êµ¬ê°€ ìˆë‹¤ë©´ ì‚¬ìš©
                pass

            # WebFetchë¡œ Wikipedia ê²€ìƒ‰
            search_term = query.replace(' ', '_').replace('?', '')
            url = f"https://en.wikipedia.org/wiki/{search_term}"

            print(f"  ğŸ“š Wikipedia ì ‘ì†...")

            # ê°„ë‹¨í•œ ìš”ì•½ ìš”ì²­
            from anthropic import Anthropic
            # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ WebFetchë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
            # ë°ëª¨ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ì‘ë‹µ ë°˜í™˜

            result = f"ê²€ìƒ‰ ê²°ê³¼: {query}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            self.search_cache[query] = result

            print(f"  âœ… ê²€ìƒ‰ ì™„ë£Œ")
            return result

        except Exception as e:
            print(f"  âš ï¸  ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def generate_response(self, prompt, max_tokens=50, temperature=1.0, top_k=50):
        """
        í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±

        Args:
            prompt: ì…ë ¥ í…ìŠ¤íŠ¸
            max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)
            top_k: Top-k ìƒ˜í”Œë§

        Returns:
            generated_text: ìƒì„±ëœ ì „ì²´ í…ìŠ¤íŠ¸
        """
        if not self.trained:
            return "âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì§„í–‰í•˜ì„¸ìš”."

        # í† í°í™”
        token_ids = self.tokenizer.encode(prompt)

        if len(token_ids) == 0:
            return "âŒ ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

        token_tensor = torch.tensor(token_ids, device=self.device).unsqueeze(0)

        generated_tokens = token_ids.copy()

        with torch.no_grad():
            for i in range(max_tokens):
                # ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if token_tensor.size(1) > self.model.pos_embedding.num_embeddings:
                    token_tensor = token_tensor[:, -self.model.pos_embedding.num_embeddings:]

                # ìˆœì „íŒŒ
                logits = self.model(token_tensor)

                # ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“
                next_token_logits = logits[0, -1, :] / temperature

                # Top-k ìƒ˜í”Œë§
                if top_k > 0:
                    # Top-k ê°’ë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” -inf
                    top_k_values, top_k_indices = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                    next_token_logits.scatter_(0, top_k_indices, top_k_values)

                # í™•ë¥  ê³„ì‚° ë° ìƒ˜í”Œë§
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1).item()

                # ìƒì„±ëœ í† í° ì¶”ê°€
                generated_tokens.append(next_token)
                token_tensor = torch.cat([
                    token_tensor,
                    torch.tensor([[next_token]], device=self.device)
                ], dim=1)

                # ì¤„ë°”ê¿ˆì´ë‚˜ ë§ˆì¹¨í‘œê°€ ì—¬ëŸ¬ ê°œ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
                if i > 10:  # ìµœì†Œ 10í† í°ì€ ìƒì„±
                    recent_text = self.tokenizer.decode(generated_tokens[-5:])
                    if recent_text.count('.') >= 2 or recent_text.count('\n') >= 2:
                        break

        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(generated_tokens)
        return generated_text

    def chat(self):
        """ì¸í„°ë™í‹°ë¸Œ ì±„íŒ… ëª¨ë“œ"""
        if not self.trained:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì§„í–‰í•˜ì„¸ìš”:")
            print("   python training_pipeline.py")
            return

        print("="*70)
        print("ğŸ’¬ SimpleLLM ì±—ë´‡")
        print("="*70)
        print()
        print("ì‚¬ìš© ë°©ë²•:")
        print("  - í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ì´ì–´ì„œ ìƒì„±í•©ë‹ˆë‹¤")
        print("  - 'quit', 'exit', 'q' ì…ë ¥ ì‹œ ì¢…ë£Œ")
        print("  - ì„¤ì • ë³€ê²½: '/temp 0.8', '/tokens 100', '/topk 30'")
        print()
        print("-"*70)

        # ê¸°ë³¸ ì„¤ì •
        max_tokens = 50
        temperature = 0.8
        top_k = 50

        while True:
            try:
                user_input = input("\nğŸ“ ì…ë ¥: ").strip()

                if not user_input:
                    print("âš ï¸  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue

                # ì¢…ë£Œ ëª…ë ¹
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                # ì„¤ì • ë³€ê²½ ëª…ë ¹
                if user_input.startswith('/'):
                    parts = user_input.split()
                    cmd = parts[0].lower()

                    if cmd == '/temp' and len(parts) > 1:
                        try:
                            temperature = float(parts[1])
                            print(f"âœ… Temperature ì„¤ì •: {temperature}")
                        except ValueError:
                            print("âŒ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

                    elif cmd == '/tokens' and len(parts) > 1:
                        try:
                            max_tokens = int(parts[1])
                            print(f"âœ… Max tokens ì„¤ì •: {max_tokens}")
                        except ValueError:
                            print("âŒ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ì •ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

                    elif cmd == '/topk' and len(parts) > 1:
                        try:
                            top_k = int(parts[1])
                            print(f"âœ… Top-k ì„¤ì •: {top_k}")
                        except ValueError:
                            print("âŒ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ì •ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

                    elif cmd == '/help':
                        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹:")
                        print("  /temp <ê°’>   - Temperature ì„¤ì • (0.1~2.0, ê¸°ë³¸: 0.8)")
                        print("  /tokens <ê°’> - ìƒì„±í•  í† í° ìˆ˜ (ê¸°ë³¸: 50)")
                        print("  /topk <ê°’>   - Top-k ìƒ˜í”Œë§ (ê¸°ë³¸: 50)")
                        print("  /help        - ë„ì›€ë§ í‘œì‹œ")

                    else:
                        print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì…ë‹ˆë‹¤. '/help'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

                    continue

                # í…ìŠ¤íŠ¸ ìƒì„±
                print("\nğŸ¤– ìƒì„± ì¤‘...", end="", flush=True)
                generated = self.generate_response(
                    user_input,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_k=top_k
                )
                print("\r" + " "*50 + "\r", end="")  # ì§„í–‰ ë©”ì‹œì§€ ì§€ìš°ê¸°

                # ê²°ê³¼ ì¶œë ¥
                print(f"ğŸ¤– ì‘ë‹µ:\n{generated}")
                print("-"*70)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()


def train_new_model():
    """ìƒˆë¡œìš´ ëª¨ë¸ í•™ìŠµ"""
    print("\nğŸš€ ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤...")
    print("ì´ ì‘ì—…ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

    try:
        import training_pipeline
        training_pipeline.main()
        print("\nâœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ì´ì œ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
        return True
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*70)
    print("ğŸ¤– SimpleLLM - ê°„ë‹¨í•œ ì–¸ì–´ ëª¨ë¸")
    print("="*70)
    print()

    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    checkpoint_path = 'checkpoints/best_model.pt'

    if not os.path.exists(checkpoint_path):
        print("âš ï¸  í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print()

        # ì‚¬ìš©ìì—ê²Œ ì„ íƒ ì œê³µ
        choice = input("ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()

        if choice == 'y':
            success = train_new_model()
            if not success:
                print("\ní•™ìŠµì„ ì‹¤í–‰í•˜ë ¤ë©´: python training_pipeline.py")
                return
        else:
            print("\ní•™ìŠµì„ ì‹¤í–‰í•˜ë ¤ë©´: python training_pipeline.py")
            return

    # ì±—ë´‡ ì‹¤í–‰
    chatbot = LLMChatbot(checkpoint_path)

    if chatbot.trained:
        chatbot.chat()
    else:
        print("\nëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
