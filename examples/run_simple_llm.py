"""
SimpleLLM ì¸í„°ë™í‹°ë¸Œ ì‹¤í–‰

ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ëª¨ë¸ì˜ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import tiktoken
import sys
from layer_normalization import SimpleLLM


def generate_text(model, tokenizer, prompt, max_new_tokens=20, temperature=1.0):
    """
    í…ìŠ¤íŠ¸ ìƒì„±

    Args:
        model: SimpleLLM ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
        temperature: ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì¶œë ¥)

    Returns:
        generated_text: ìƒì„±ëœ í…ìŠ¤íŠ¸
    """
    model.eval()

    # í”„ë¡¬í”„íŠ¸ í† í°í™”
    token_ids = tokenizer.encode(prompt)
    token_tensor = torch.tensor(token_ids).unsqueeze(0)  # [1, seq_len]

    generated_tokens = token_ids.copy()

    print(f"\nìƒì„± ì¤‘", end="", flush=True)

    with torch.no_grad():
        for _ in range(max_new_tokens):
            # í˜„ì¬ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡
            logits = model(token_tensor)

            # ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“
            last_logits = logits[0, -1, :] / temperature

            # í™•ë¥  ê³„ì‚° ë° ìƒ˜í”Œë§
            probs = torch.softmax(last_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).item()

            # ìƒì„±ëœ í† í° ì¶”ê°€
            generated_tokens.append(next_token)
            token_tensor = torch.tensor(generated_tokens).unsqueeze(0)

            # ì§„í–‰ í‘œì‹œ
            print(".", end="", flush=True)

            # ì¢…ë£Œ í† í° ì²´í¬ (optional)
            if next_token == tokenizer.eot_token:
                break

    print(" ì™„ë£Œ!\n")

    # ë””ì½”ë”©
    generated_text = tokenizer.decode(generated_tokens)
    return generated_text


def analyze_next_token(model, tokenizer, text, top_k=10):
    """
    ë‹¤ìŒ í† í° ì˜ˆì¸¡ ë¶„ì„

    Args:
        model: SimpleLLM ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        top_k: ìƒìœ„ kê°œ í† í° í‘œì‹œ
    """
    model.eval()

    # í† í°í™”
    token_ids = tokenizer.encode(text)
    tokens = [tokenizer.decode([tid]) for tid in token_ids]
    token_tensor = torch.tensor(token_ids).unsqueeze(0)

    print(f"\nğŸ“ ì…ë ¥: \"{text}\"")
    print(f"í† í° ê°œìˆ˜: {len(token_ids)}")
    print(f"í† í°: {tokens}\n")

    # ìˆœì „íŒŒ
    with torch.no_grad():
        logits = model(token_tensor)

    # ë§ˆì§€ë§‰ í† í°ì˜ ì˜ˆì¸¡
    last_logits = logits[0, -1, :]
    probs = torch.softmax(last_logits, dim=-1)

    # Top-k í† í°
    topk_probs, topk_indices = torch.topk(probs, top_k)

    print(f"ğŸ¯ ë‹¤ìŒ í† í° ì˜ˆì¸¡ (Top {top_k}):")
    print("-" * 60)
    for i, (prob, idx) in enumerate(zip(topk_probs, topk_indices), 1):
        token = tokenizer.decode([idx.item()])
        bar_length = int(prob.item() * 100)
        bar = "â–ˆ" * (bar_length // 2)
        print(f"{i:2d}. '{token:15s}' {prob.item()*100:6.2f}% {bar}")


def main():
    print("="*80)
    print("ğŸ¤– SimpleLLM - ê°„ë‹¨í•œ ì–¸ì–´ ëª¨ë¸")
    print("="*80)
    print()

    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
    tokenizer = tiktoken.get_encoding("gpt2")
    vocab_size = tokenizer.n_vocab

    torch.manual_seed(42)
    model = SimpleLLM(
        vocab_size=vocab_size,
        d_embed=256,
        num_heads=4,
        num_layers=4,
        max_seq_len=512,
        dropout=0.1
    )

    print("ğŸ’¡ ì°¸ê³ : ì´ ëª¨ë¸ì€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ë¬´ì‘ìœ„ ê°€ì¤‘ì¹˜).")
    print("   í•™ìŠµëœ ëª¨ë¸ì´ë¼ë©´ ë” ì˜ë¯¸ìˆëŠ” ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

    # ëª…ë ¹ì¤„ ì¸ì í™•ì¸
    if len(sys.argv) > 1:
        mode = sys.argv[1]

        if mode == "generate":
            # í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë“œ
            prompt = " ".join(sys.argv[2:]) if len(sys.argv) > 2 else "Hello"
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: \"{prompt}\"")
            generated = generate_text(model, tokenizer, prompt, max_new_tokens=20)
            print(f"âœ¨ ìƒì„±ëœ í…ìŠ¤íŠ¸:\n{generated}\n")

        elif mode == "analyze":
            # ë‹¤ìŒ í† í° ë¶„ì„ ëª¨ë“œ
            text = " ".join(sys.argv[2:]) if len(sys.argv) > 2 else "Hello"
            analyze_next_token(model, tokenizer, text, top_k=10)

        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {mode}")
            print("ì‚¬ìš©ë²•: python run_simple_llm.py [generate|analyze] 'your text'")

    else:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        print("ğŸ’¡ ì‚¬ìš©ë²•:")
        print("  - 'analyze: your text' - ë‹¤ìŒ í† í° ì˜ˆì¸¡ ë¶„ì„")
        print("  - 'generate: your text' - í…ìŠ¤íŠ¸ ìƒì„±")
        print("  - 'quit' ë˜ëŠ” 'exit' - ì¢…ë£Œ")
        print()

        while True:
            try:
                user_input = input("ğŸ“ ì…ë ¥: ").strip()

                if not user_input:
                    print("âš ï¸  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                # ëª…ë ¹ íŒŒì‹±
                if user_input.startswith("analyze:"):
                    text = user_input[8:].strip()
                    if text:
                        analyze_next_token(model, tokenizer, text, top_k=10)
                    else:
                        print("âš ï¸  ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

                elif user_input.startswith("generate:"):
                    text = user_input[9:].strip()
                    if text:
                        generated = generate_text(model, tokenizer, text, max_new_tokens=20)
                        print(f"âœ¨ ìƒì„±ëœ í…ìŠ¤íŠ¸:\n{generated}\n")
                    else:
                        print("âš ï¸  ìƒì„±í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

                else:
                    # ê¸°ë³¸ê°’: ë¶„ì„
                    analyze_next_token(model, tokenizer, user_input, top_k=10)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}\n")


if __name__ == "__main__":
    main()
