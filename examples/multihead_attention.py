import torch
import torch.nn as nn
import tiktoken
import sys


class TokenEmbedding(nn.Module):
    """í† í° IDë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""

    def __init__(self, vocab_size, d_embed):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_embed)

    def forward(self, token_ids):
        return self.embedding(token_ids)


class MultiHeadAttention(nn.Module):
    """
    ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (Multi-Head Attention)

    ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ í—¤ë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ í•™ìŠµ

    ì‘ë™ ì›ë¦¬:
    1. ì…ë ¥ì„ num_headsê°œë¡œ ë¶„í• 
    2. ê° í—¤ë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ì–´í…ì…˜ ê³„ì‚°
    3. ëª¨ë“  í—¤ë“œì˜ ê²°ê³¼ë¥¼ í•©ì¹¨(concatenate)
    4. ìµœì¢… ì„ í˜• ë³€í™˜ ì ìš©
    """

    def __init__(self, d_in, d_out, num_heads, dropout=0.0, qkv_bias=False):
        """
        Args:
            d_in: ì…ë ¥ ì°¨ì›
            d_out: ì¶œë ¥ ì°¨ì›
            num_heads: í—¤ë“œ ê°œìˆ˜
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            qkv_bias: Query, Key, Value ë ˆì´ì–´ì— bias ì‚¬ìš© ì—¬ë¶€
        """
        super().__init__()

        assert d_out % num_heads == 0, "d_outì€ num_headsë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # ê° í—¤ë“œì˜ ì°¨ì›

        # Query, Key, Value ë³€í™˜ ë ˆì´ì–´
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # ìµœì¢… ì¶œë ¥ ë³€í™˜ ë ˆì´ì–´
        self.out_proj = nn.Linear(d_out, d_out)
        self.dropout = nn.Dropout(dropout)

        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì €ì¥ (ì‹œê°í™”ìš©)
        self.attn_weights = None

    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, d_in]

        Returns:
            output: [batch_size, seq_len, d_out]
        """
        batch_size, seq_len, d_in = x.shape

        # Step 1: Query, Key, Value ê³„ì‚°
        queries = self.W_query(x)  # [batch_size, seq_len, d_out]
        keys = self.W_key(x)       # [batch_size, seq_len, d_out]
        values = self.W_value(x)   # [batch_size, seq_len, d_out]

        # Step 2: ë©€í‹°-í—¤ë“œë¡œ ë¶„í• 
        # [batch_size, seq_len, d_out] â†’ [batch_size, seq_len, num_heads, head_dim]
        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim)
        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim)
        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim)

        # ì°¨ì› ì¬ë°°ì¹˜: [batch_size, num_heads, seq_len, head_dim]
        queries = queries.transpose(1, 2)
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)

        # Step 3: ê° í—¤ë“œë³„ë¡œ Scaled Dot-Product Attention ê³„ì‚°
        # [batch_size, num_heads, seq_len, seq_len]
        attn_scores = queries @ keys.transpose(2, 3)

        # Scaling
        attn_scores = attn_scores / (self.head_dim ** 0.5)

        # Causal mask ì ìš© (í˜„ì¬ ìœ„ì¹˜ë³´ë‹¤ ë’¤ì˜ í† í°ì€ ë³´ì§€ ëª»í•˜ê²Œ)
        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)
        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))

        # Softmax
        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # ì‹œê°í™”ë¥¼ ìœ„í•´ ì €ì¥
        self.attn_weights = attn_weights.detach()

        # Step 4: Valueì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì ìš©
        # [batch_size, num_heads, seq_len, head_dim]
        context = attn_weights @ values

        # Step 5: í—¤ë“œë“¤ì„ ë‹¤ì‹œ í•©ì¹¨ (concatenate)
        # [batch_size, seq_len, num_heads, head_dim]
        context = context.transpose(1, 2)

        # [batch_size, seq_len, d_out]
        context = context.contiguous().view(batch_size, seq_len, self.d_out)

        # Step 6: ìµœì¢… ì„ í˜• ë³€í™˜
        output = self.out_proj(context)

        return output


class MultiHeadAttentionModel:
    """ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ëª¨ë¸"""

    def __init__(self, d_embed=128, d_out=128, num_heads=24, dropout=0.1):
        """
        Args:
            d_embed: ì„ë² ë”© ì°¨ì›
            d_out: ì¶œë ¥ ì°¨ì›
            num_heads: í—¤ë“œ ê°œìˆ˜ (d_outì„ num_headsë¡œ ë‚˜ëˆˆ ê°’ì´ ê° í—¤ë“œì˜ ì°¨ì›)
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        """
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.vocab_size = self.tokenizer.n_vocab
        self.num_heads = num_heads

        self.embedding = TokenEmbedding(self.vocab_size, d_embed)
        self.attention = MultiHeadAttention(
            d_in=d_embed,
            d_out=d_out,
            num_heads=num_heads,
            dropout=dropout
        )

        print(f"âœ… ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì–´íœ˜ í¬ê¸°: {self.vocab_size}")
        print(f"   - ì„ë² ë”© ì°¨ì›: {d_embed}")
        print(f"   - ì¶œë ¥ ì°¨ì›: {d_out}")
        print(f"   - í—¤ë“œ ê°œìˆ˜: {num_heads}")
        print(f"   - ê° í—¤ë“œ ì°¨ì›: {d_out // num_heads}")
        print(f"   - ë“œë¡­ì•„ì›ƒ: {dropout}\n")

    def process(self, text):
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        # í† í°í™”
        token_ids = self.tokenizer.encode(text)
        tokens = [self.tokenizer.decode([tid]) for tid in token_ids]

        # í…ì„œë¡œ ë³€í™˜
        token_tensor = torch.tensor(token_ids).unsqueeze(0)

        # ì„ë² ë”© + ë©€í‹°-í—¤ë“œ ì–´í…ì…˜
        embeddings = self.embedding(token_tensor)
        output = self.attention(embeddings)

        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸°
        attn_weights = self.attention.attn_weights

        return output, tokens, attn_weights

    def visualize_all_heads(self, tokens, attn_weights):
        """ëª¨ë“  í—¤ë“œì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        print(f"\n{'='*80}")
        print(f"ğŸ” ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™” (ì´ {self.num_heads}ê°œ í—¤ë“œ)")
        print(f"{'='*80}\n")

        # attn_weights shape: [batch_size, num_heads, seq_len, seq_len]
        attn_matrix = attn_weights[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ

        for head_idx in range(self.num_heads):
            print(f"\n[ Head {head_idx + 1}/{self.num_heads} ]")
            print("-" * 80)
            self._visualize_single_head(tokens, attn_matrix[head_idx])

    def _visualize_single_head(self, tokens, attn_matrix):
        """ë‹¨ì¼ í—¤ë“œì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        attn_np = attn_matrix.numpy()

        # í—¤ë”
        max_len = max(len(t) for t in tokens)
        header = " " * (max_len + 2)
        for token in tokens:
            header += f"{token[:8]:>10}"
        print(header)
        print("-" * len(header))

        # ê° í–‰
        for i, token in enumerate(tokens):
            row = f"{token[:max_len]:<{max_len+2}}"
            for j in range(len(tokens)):
                if j <= i:
                    row += f"{attn_np[i, j]:>10.3f}"
                else:
                    row += f"{'---':>10}"
            print(row)

    def visualize_average_attention(self, tokens, attn_weights):
        """ëª¨ë“  í—¤ë“œì˜ í‰ê·  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        print(f"\n{'='*80}")
        print("ğŸ“Š í‰ê·  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ëª¨ë“  í—¤ë“œì˜ í‰ê· )")
        print(f"{'='*80}")

        # ëª¨ë“  í—¤ë“œì˜ í‰ê·  ê³„ì‚°
        attn_matrix = attn_weights[0].mean(dim=0)  # [seq_len, seq_len]

        attn_np = attn_matrix.numpy()

        # í—¤ë”
        max_len = max(len(t) for t in tokens)
        header = " " * (max_len + 2)
        for token in tokens:
            header += f"{token[:8]:>10}"
        print(header)
        print("-" * len(header))

        # ê° í–‰
        for i, token in enumerate(tokens):
            row = f"{token[:max_len]:<{max_len+2}}"
            for j in range(len(tokens)):
                if j <= i:
                    row += f"{attn_np[i, j]:>10.3f}"
                else:
                    row += f"{'---':>10}"
            print(row)
        print()

    def compare_heads(self, tokens, attn_weights):
        """ê° í—¤ë“œê°€ íŠ¹ì • í† í°ì— ì§‘ì¤‘í•˜ëŠ” ì •ë„ ë¹„êµ"""
        print(f"\n{'='*80}")
        print("ğŸ¯ ê° í—¤ë“œê°€ ê° í† í°ì— ì§‘ì¤‘í•˜ëŠ” ì •ë„ (ë§ˆì§€ë§‰ í† í° ê¸°ì¤€)")
        print(f"{'='*80}\n")

        # ë§ˆì§€ë§‰ í† í°ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ [num_heads, seq_len]
        last_token_attn = attn_weights[0, :, -1, :]

        print(f"{'í—¤ë“œ':<8}", end="")
        for token in tokens:
            print(f"{token[:8]:>10}", end="")
        print("\n" + "-" * (8 + 10 * len(tokens)))

        for head_idx in range(self.num_heads):
            print(f"Head {head_idx+1:<3}", end="")
            for token_idx in range(len(tokens)):
                print(f"{last_token_attn[head_idx, token_idx]:>10.3f}", end="")
            print()
        print()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*80)
    print("ğŸ¤– ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ëª¨ë¸ - í…ìŠ¤íŠ¸ ì…ë ¥ ë°›ê¸°")
    print("="*80)
    print()

    # ëª¨ë¸ ì´ˆê¸°í™”
    torch.manual_seed(42)
    model = MultiHeadAttentionModel(
        d_embed=512,
        d_out=128,
        num_heads=8,  # 4ê°œì˜ í—¤ë“œ ì‚¬ìš©
        dropout=0.1
    )

    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìë¡œ ì…ë ¥ ë°›ê¸°
    if len(sys.argv) > 1:
        # ëª…ë ¹ì–´ë¡œ ì‹¤í–‰: python multihead_attention.py "your text here"
        text = " ".join(sys.argv[1:])
        print(f"ğŸ“ ì…ë ¥: \"{text}\"\n")

        # ì²˜ë¦¬
        output, tokens, attn_weights = model.process(text)

        print(f"í† í° ê°œìˆ˜: {len(tokens)}")
        print(f"í† í°: {tokens}")
        print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")

        # ì‹œê°í™”
        model.visualize_all_heads(tokens, attn_weights)
        model.visualize_average_attention(tokens, attn_weights)
        model.compare_heads(tokens, attn_weights)

    else:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        print("ğŸ’¡ ì‚¬ìš©ë²•: python multihead_attention.py \"your text here\"")
        print("ë˜ëŠ” ì•„ë˜ì— ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”.\n")

        while True:
            try:
                text = input("ğŸ“ í…ìŠ¤íŠ¸ ì…ë ¥ (ì¢…ë£Œ: Ctrl+C): ").strip()

                if not text:
                    print("âš ï¸  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                # ì²˜ë¦¬
                output, tokens, attn_weights = model.process(text)

                print(f"\ní† í° ê°œìˆ˜: {len(tokens)}")
                print(f"í† í°: {tokens}")
                print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")

                # ì‹œê°í™”
                model.visualize_all_heads(tokens, attn_weights)
                model.visualize_average_attention(tokens, attn_weights)
                model.compare_heads(tokens, attn_weights)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}\n")


if __name__ == "__main__":
    main()
