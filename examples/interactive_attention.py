import torch
import torch.nn as nn
import tiktoken


class TokenEmbedding(nn.Module):
    """í† í° IDë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""

    def __init__(self, vocab_size, d_embed):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_embed)

    def forward(self, token_ids):
        return self.embedding(token_ids)


class CausalAttention(nn.Module):
    """ì¸ê³¼ì  Self-Attention (GPT ìŠ¤íƒ€ì¼)"""

    def __init__(self, d_in, d_out):
        super().__init__()
        self.d_out = d_out

        self.W_query = nn.Linear(d_in, d_out, bias=False)
        self.W_key = nn.Linear(d_in, d_out, bias=False)
        self.W_value = nn.Linear(d_in, d_out, bias=False)

    def forward(self, x, verbose=False):
        batch_size, seq_len, d_in = x.shape

        queries = self.W_query(x)
        keys = self.W_key(x)
        values = self.W_value(x)

        # Attention scores ê³„ì‚°
        attn_scores = queries @ keys.transpose(1, 2)
        attn_scores_scaled = attn_scores / (self.d_out ** 0.5)

        # Causal mask ì ìš©
        mask = torch.tril(torch.ones(seq_len, seq_len))
        attn_scores_masked = attn_scores_scaled.masked_fill(mask == 0, float('-inf'))

        # Softmaxë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attn_weights = torch.softmax(attn_scores_masked, dim=-1)

        if verbose:
            print(f"\n[ Attention Weights ]")
            print("ê° í† í°ì´ ì´ì „ í† í°ë“¤ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€:")
            print(attn_weights[0].detach().numpy())

        # Context vector ê³„ì‚°
        context = attn_weights @ values

        return context, attn_weights


class InteractiveAttentionModel:
    """
    ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì–´í…ì…˜ì„ ì ìš©í•˜ëŠ” ì¸í„°ë™í‹°ë¸Œ ëª¨ë¸
    """

    def __init__(self, d_embed=128, d_out=128):
        """
        Args:
            d_embed: ì„ë² ë”© ì°¨ì›
            d_out: ì–´í…ì…˜ ì¶œë ¥ ì°¨ì›
        """
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.vocab_size = self.tokenizer.n_vocab

        # ì„ë² ë”© ë ˆì´ì–´
        self.embedding = TokenEmbedding(self.vocab_size, d_embed)

        # ì–´í…ì…˜ ë ˆì´ì–´
        self.attention = CausalAttention(d_embed, d_out)

        print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì–´íœ˜ í¬ê¸°: {self.vocab_size}")
        print(f"   - ì„ë² ë”© ì°¨ì›: {d_embed}")
        print(f"   - ì–´í…ì…˜ ì¶œë ¥ ì°¨ì›: {d_out}\n")

    def process_text(self, text, verbose=True):
        """
        ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì–´í…ì…˜ ì ìš©

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            verbose: ìƒì„¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€

        Returns:
            output: ì–´í…ì…˜ì´ ì ìš©ëœ ì¶œë ¥ ë²¡í„°
            tokens: í† í° ë¦¬ìŠ¤íŠ¸
            attn_weights: ì–´í…ì…˜ ê°€ì¤‘ì¹˜
        """
        # 1. í† í°í™”
        token_ids = self.tokenizer.encode(text)
        tokens = [self.tokenizer.decode([tid]) for tid in token_ids]

        if verbose:
            print(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: \"{text}\"")
            print(f"\n[ Step 1: í† í°í™” ]")
            print(f"í† í° ê°œìˆ˜: {len(token_ids)}")
            print(f"í† í° ID: {token_ids}")
            print(f"í† í°: {tokens}")

        # 2. í…ì„œë¡œ ë³€í™˜ (ë°°ì¹˜ ì°¨ì› ì¶”ê°€)
        token_tensor = torch.tensor(token_ids).unsqueeze(0)  # [1, seq_len]

        # 3. ì„ë² ë”©
        embeddings = self.embedding(token_tensor)  # [1, seq_len, d_embed]

        if verbose:
            print(f"\n[ Step 2: ì„ë² ë”© ]")
            print(f"ì„ë² ë”© í¬ê¸°: {embeddings.shape}")
            print(f"ì²« ë²ˆì§¸ í† í°ì˜ ì„ë² ë”© ë²¡í„° (ì²˜ìŒ 5ê°œ ì°¨ì›):")
            print(f"{embeddings[0, 0, :5].detach().numpy()}")

        # 4. ì–´í…ì…˜ ì ìš©
        output, attn_weights = self.attention(embeddings, verbose=verbose)

        if verbose:
            print(f"\n[ Step 3: ì–´í…ì…˜ ì ìš© ]")
            print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")
            print(f"ì²« ë²ˆì§¸ í† í°ì˜ ì¶œë ¥ ë²¡í„° (ì²˜ìŒ 5ê°œ ì°¨ì›):")
            print(f"{output[0, 0, :5].detach().numpy()}")

        return output, tokens, attn_weights

    def visualize_attention(self, tokens, attn_weights):
        """
        ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”

        Args:
            tokens: í† í° ë¦¬ìŠ¤íŠ¸
            attn_weights: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ [1, seq_len, seq_len]
        """
        print(f"\n{'='*70}")
        print("ğŸ” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
        print(f"{'='*70}")
        print("ê° í† í°(í–‰)ì´ ì´ì „ í† í°ë“¤(ì—´)ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        print("ìˆ«ìê°€ í´ìˆ˜ë¡ ë” ë§ì´ ì§‘ì¤‘í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n")

        attn_matrix = attn_weights[0].detach().numpy()

        # í—¤ë” ì¶œë ¥
        max_token_len = max(len(t) for t in tokens)
        header = " " * (max_token_len + 2)
        for token in tokens:
            header += f"{token[:6]:>8}"
        print(header)
        print("-" * len(header))

        # ê° í–‰ ì¶œë ¥
        for i, token in enumerate(tokens):
            row = f"{token[:max_token_len]:<{max_token_len+2}}"
            for j in range(len(tokens)):
                if j <= i:  # Causal mask: í˜„ì¬ì™€ ì´ì „ í† í°ë§Œ
                    row += f"{attn_matrix[i, j]:>8.3f}"
                else:
                    row += f"{'---':>8}"
            print(row)

    def compare_tokens(self, tokens, output):
        """
        ê° í† í°ì˜ ì¶œë ¥ ë²¡í„° ë¹„êµ

        Args:
            tokens: í† í° ë¦¬ìŠ¤íŠ¸
            output: ì–´í…ì…˜ ì¶œë ¥ [1, seq_len, d_out]
        """
        print(f"\n{'='*70}")
        print("ğŸ“Š ê° í† í°ì˜ ì¶œë ¥ ë²¡í„° ë¹„êµ (ì²˜ìŒ 10ê°œ ì°¨ì›)")
        print(f"{'='*70}\n")

        output_matrix = output[0].detach().numpy()

        for i, token in enumerate(tokens):
            print(f"í† í° {i+1}: '{token}'")
            print(f"  ì¶œë ¥: {output_matrix[i, :10]}")
            print()


def interactive_mode():
    """ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œë¡œ ì‹¤í–‰"""
    print("="*70)
    print("ğŸ¤– ì¸í„°ë™í‹°ë¸Œ ì–´í…ì…˜ ëª¨ë¸")
    print("="*70)
    print()

    # ëª¨ë¸ ì´ˆê¸°í™”
    torch.manual_seed(42)
    model = InteractiveAttentionModel(d_embed=128, d_out=128)

    while True:
        print("\n" + "="*70)
        user_input = input("ğŸ“ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit'): ").strip()

        if user_input.lower() in ['quit', 'exit', 'q']:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not user_input:
            print("âš ï¸  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue

        try:
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬
            output, tokens, attn_weights = model.process_text(user_input, verbose=True)

            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
            model.visualize_attention(tokens, attn_weights)

            # í† í°ë³„ ì¶œë ¥ ë¹„êµ
            model.compare_tokens(tokens, output)

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


def example_mode():
    """ì˜ˆì œ ëª¨ë“œë¡œ ì‹¤í–‰"""
    print("="*70)
    print("ğŸ“š ì˜ˆì œ ëª¨ë“œ: ë¯¸ë¦¬ ì •ì˜ëœ í…ìŠ¤íŠ¸ë¡œ ì‹¤í–‰")
    print("="*70)
    print()

    # ëª¨ë¸ ì´ˆê¸°í™”
    torch.manual_seed(42)
    model = InteractiveAttentionModel(d_embed=128, d_out=128)

    # ì˜ˆì œ í…ìŠ¤íŠ¸ë“¤
    examples = [
        "Hello, how are you?",
        "I love machine learning",
        "The quick brown fox jumps over the lazy dog",
        "Attention is all you need"
    ]

    for idx, text in enumerate(examples, 1):
        print(f"\n{'='*70}")
        print(f"ì˜ˆì œ {idx}/{len(examples)}")
        print(f"{'='*70}")

        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        output, tokens, attn_weights = model.process_text(text, verbose=True)

        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
        model.visualize_attention(tokens, attn_weights)

        if idx < len(examples):
            input("\nâ¸ï¸  ë‹¤ìŒ ì˜ˆì œë¥¼ ë³´ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")


if __name__ == "__main__":
    print("\nì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ (ì§ì ‘ ì…ë ¥)")
    print("2. ì˜ˆì œ ëª¨ë“œ (ë¯¸ë¦¬ ì •ì˜ëœ í…ìŠ¤íŠ¸)")
    print()

    choice = input("ì„ íƒ (1 ë˜ëŠ” 2): ").strip()

    if choice == "1":
        interactive_mode()
    elif choice == "2":
        example_mode()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
