"""
Web-Enhanced LLM Chatbot

í•™ìŠµ ë°ì´í„°ì— ë‹µë³€ì´ ì—†ìœ¼ë©´ ì›¹ì—ì„œ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•˜ëŠ” ì±—ë´‡
"""

import torch
import torch.nn as nn
import tiktoken
import os
import sys
import re
from datetime import datetime

from layer_normalization import SimpleLLM


class WebEnhancedChatbot:
    """ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ LLM ì±—ë´‡"""

    def __init__(self, checkpoint_path='checkpoints/best_model.pt'):
        """
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        """
        print("="*70)
        print("ğŸ¤– Web-Enhanced SimpleLLM ì±—ë´‡")
        print("="*70)
        print()

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print("âœ… GPU ì‚¬ìš© (CUDA)")
        elif torch.backends.mps.is_available():
            self.device = torch.device('mps')
            print("âœ… GPU ì‚¬ìš© (Apple Silicon)")
        else:
            self.device = torch.device('cpu')
            print("âš ï¸  CPU ì‚¬ìš©")

        # í† í¬ë‚˜ì´ì €
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.vocab_size = self.tokenizer.n_vocab

        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        if not os.path.exists(checkpoint_path):
            print(f"\nâš ï¸  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            print("ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (ì›¹ ê²€ìƒ‰ë§Œ ì‚¬ìš©).\n")
            self.model = None
            self.trained = False
        else:
            # ëª¨ë¸ ë¡œë“œ
            print(f"\nğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            config = checkpoint.get('config', {})

            self.model = SimpleLLM(
                vocab_size=self.vocab_size,
                d_embed=config.get('d_embed', 256),
                num_heads=config.get('num_heads', 4),
                num_layers=config.get('num_layers', 4),
                max_seq_len=config.get('max_seq_len', 256),
                dropout=0.0
            ).to(self.device)

            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()

            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")
            self.trained = True

        # ì›¹ ê²€ìƒ‰ ìºì‹œ (ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€)
        self.search_cache = {}

        # ì§€ì‹ ë² ì´ìŠ¤ (ê²€ìƒ‰ ê²°ê³¼ ì €ì¥)
        self.knowledge_base = {}

    def is_question(self, text):
        """
        ì…ë ¥ì´ ì§ˆë¬¸ì¸ì§€ íŒë‹¨

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            bool: ì§ˆë¬¸ ì—¬ë¶€
        """
        # ì§ˆë¬¸ íŒ¨í„´ ê°ì§€
        question_patterns = [
            r'\?$',  # ë¬¼ìŒí‘œë¡œ ëë‚¨
            r'^(what|where|when|who|why|how|which|is|are|do|does|can|could|would|should)',  # ì§ˆë¬¸ ì‹œì‘
            r'(ë¬´ì—‡|ì–´ë””|ì–¸ì œ|ëˆ„êµ¬|ì™œ|ì–´ë–»ê²Œ|ë¬´ìŠ¨|ì–´ëŠ)',  # í•œê¸€ ì§ˆë¬¸
            r'(ì¸ê°€ìš”|ì¸ì§€|ì…ë‹ˆê¹Œ|ì¸ê°€|í• ê¹Œ|ã„¹ê¹Œ|ë©´)$'  # í•œê¸€ ì§ˆë¬¸ ì¢…ê²°
        ]

        text_lower = text.lower().strip()

        for pattern in question_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return True

        return False

    def generate_response(self, prompt, max_tokens=50, temperature=0.8):
        """
        ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„± (ê¸°ì¡´ ë°©ì‹)

        Args:
            prompt: ì…ë ¥ í…ìŠ¤íŠ¸
            max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            generated_text: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        if not self.trained:
            return None

        try:
            token_ids = self.tokenizer.encode(prompt)
            if len(token_ids) == 0:
                return None

            token_tensor = torch.tensor(token_ids, device=self.device).unsqueeze(0)
            generated_tokens = token_ids.copy()

            with torch.no_grad():
                for i in range(max_tokens):
                    if token_tensor.size(1) > self.model.pos_embedding.num_embeddings:
                        token_tensor = token_tensor[:, -self.model.pos_embedding.num_embeddings:]

                    logits = self.model(token_tensor)
                    next_token_logits = logits[0, -1, :] / temperature

                    # Top-k ìƒ˜í”Œë§
                    top_k = 50
                    top_k_values, top_k_indices = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                    next_token_logits.scatter_(0, top_k_indices, top_k_values)

                    probs = torch.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1).item()

                    generated_tokens.append(next_token)
                    token_tensor = torch.cat([
                        token_tensor,
                        torch.tensor([[next_token]], device=self.device)
                    ], dim=1)

                    # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
                    if i > 10:
                        recent_text = self.tokenizer.decode(generated_tokens[-5:])
                        if recent_text.count('.') >= 2 or recent_text.count('\n') >= 2:
                            break

            generated_text = self.tokenizer.decode(generated_tokens)
            return generated_text

        except Exception as e:
            print(f"âš ï¸  ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def is_response_valid(self, original_prompt, response):
        """
        ìƒì„±ëœ ì‘ë‹µì´ ìœ íš¨í•œì§€ íŒë‹¨

        Args:
            original_prompt: ì›ë³¸ ì§ˆë¬¸
            response: ìƒì„±ëœ ì‘ë‹µ

        Returns:
            bool: ìœ íš¨ ì—¬ë¶€
        """
        if not response or response == original_prompt:
            return False

        # ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ê²½ìš°
        response_only = response[len(original_prompt):].strip()
        if len(response_only) < 10:
            return False

        # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ê°€ ë„ˆë¬´ ë§ì€ ê²½ìš°
        words = response_only.split()
        if len(words) > 5:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.3:  # 70% ì´ìƒ ë°˜ë³µ
                return False

        return True

    def search_web(self, query):
        """
        ì›¹ì—ì„œ ì •ë³´ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬

        Returns:
            result: ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸
        """
        # ìºì‹œ í™•ì¸
        if query in self.search_cache:
            print("ğŸ“¦ ìºì‹œì—ì„œ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return self.search_cache[query]

        print(f"\nğŸ” ì›¹ ê²€ìƒ‰ ì¤‘: '{query}'")

        # ê°„ë‹¨í•œ ì§€ì‹ ë² ì´ìŠ¤ (ë°ëª¨ìš©)
        # ì‹¤ì œë¡œëŠ” ì›¹ APIë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
        result = self._search_from_knowledge_base(query)

        if result:
            self.search_cache[query] = result
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ!")
            return result
        else:
            print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë” ë‚˜ì€ ê²€ìƒ‰ì„ ìœ„í•´ ë” í° ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•˜ê±°ë‚˜,")
            print("   ì™¸ë¶€ API (OpenAI, Google Search API ë“±)ë¥¼ ì—°ë™í•˜ì„¸ìš”.")
            return None

    def _process_search_results(self, results):
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬

        Args:
            results: ê²€ìƒ‰ ê²°ê³¼

        Returns:
            processed_text: ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¬¸ìì—´ì¸ ê²½ìš°
        if isinstance(results, str):
            return results[:2000]  # ìµœëŒ€ 2000ì

        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        if isinstance(results, list):
            texts = []
            for item in results[:3]:  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ
                if isinstance(item, dict):
                    title = item.get('title', '')
                    content = item.get('content', '') or item.get('snippet', '')
                    texts.append(f"{title}: {content}")
                elif isinstance(item, str):
                    texts.append(item)

            return "\n\n".join(texts)[:2000]

        return str(results)[:2000]

    def _search_from_knowledge_base(self, query):
        """
        ë‚´ì¥ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ (ë°ëª¨ìš©)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬

        Returns:
            result: ê²€ìƒ‰ ê²°ê³¼
        """
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì§€ì‹ ë² ì´ìŠ¤
        knowledge = {
            # AI ê´€ë ¨
            "artificial intelligence": "Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction. AI is used in various applications such as speech recognition, image processing, and autonomous vehicles.",

            "machine learning": "Machine Learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.",

            "deep learning": "Deep Learning is a subset of machine learning based on artificial neural networks. It uses multiple layers to progressively extract higher-level features from raw input. Deep learning is particularly effective for tasks like image and speech recognition.",

            "neural network": "A Neural Network is a series of algorithms that attempts to recognize underlying relationships in data through a process that mimics the way the human brain operates. It consists of interconnected nodes (neurons) organized in layers.",

            # í”„ë¡œê·¸ë˜ë° ê´€ë ¨
            "python": "Python is a high-level, interpreted programming language known for its simplicity and readability. It's widely used in web development, data science, machine learning, and automation. Python emphasizes code readability and supports multiple programming paradigms.",

            "pytorch": "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab. It's widely used for applications such as computer vision and natural language processing. PyTorch provides flexibility and speed in building deep learning models.",

            # ì¼ë°˜ ì§€ì‹
            "transformer": "Transformer is a deep learning model architecture introduced in 2017. It uses self-attention mechanisms to process sequential data and has become the foundation for modern language models like GPT and BERT.",

            "llm": "Large Language Model (LLM) is an AI model trained on vast amounts of text data to understand and generate human-like text. Examples include GPT-3, GPT-4, and BERT. LLMs can perform various tasks like translation, summarization, and question answering.",
        }

        # ì¿¼ë¦¬ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰
        query_lower = query.lower()

        # ì§ì ‘ ë§¤ì¹­
        for key, value in knowledge.items():
            if key in query_lower or query_lower in key:
                return value

        # í‚¤ì›Œë“œ ë§¤ì¹­
        keywords = query_lower.split()
        best_match = None
        max_score = 0

        for key, value in knowledge.items():
            score = sum(1 for keyword in keywords if keyword in key)
            if score > max_score:
                max_score = score
                best_match = value

        if max_score > 0:
            return best_match

        return None

    def _fallback_wikipedia_search(self, query):
        """
        Wikipediaì—ì„œ ì •ë³´ ê²€ìƒ‰ (fallback)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬

        Returns:
            result: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            from tools import WebFetch

            # Wikipedia URL ìƒì„±
            search_term = query.replace(' ', '_')
            url = f"https://en.wikipedia.org/wiki/{search_term}"

            print(f"ğŸ“š Wikipedia ê²€ìƒ‰: {url}")

            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            result = WebFetch(
                url=url,
                prompt=f"Summarize the main information about {query} in 2-3 sentences."
            )

            if result:
                print(f"âœ… Wikipediaì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                return result[:2000]
            else:
                return None

        except Exception as e:
            print(f"âš ï¸  Wikipedia ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def answer_with_web(self, question):
        """
        ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±

        Args:
            question: ì§ˆë¬¸

        Returns:
            answer: ë‹µë³€
        """
        # ì›¹ ê²€ìƒ‰
        search_result = self.search_web(question)

        if not search_result:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§€ì‹ ë² ì´ìŠ¤ì— ì €ì¥
        self.knowledge_base[question] = {
            'content': search_result,
            'timestamp': datetime.now().isoformat()
        }

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        answer = self._summarize_search_result(search_result, question)

        return answer

    def _summarize_search_result(self, content, question):
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½

        Args:
            content: ê²€ìƒ‰ ê²°ê³¼ ë‚´ìš©
            question: ì›ë³¸ ì§ˆë¬¸

        Returns:
            summary: ìš”ì•½ëœ ë‹µë³€
        """
        # ê°„ë‹¨í•œ ìš”ì•½: ì²« ëª‡ ë¬¸ì¥ ì¶”ì¶œ
        sentences = content.split('.')
        relevant_sentences = []

        # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì¥ ì°¾ê¸°
        question_keywords = set(question.lower().split())

        for sentence in sentences[:10]:  # ì²« 10ê°œ ë¬¸ì¥ë§Œ í™•ì¸
            sentence_lower = sentence.lower()
            # ì§ˆë¬¸ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ìš°ì„ 
            if any(keyword in sentence_lower for keyword in question_keywords if len(keyword) > 3):
                relevant_sentences.append(sentence.strip())

        # ê´€ë ¨ ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì²« 2-3ê°œ ë¬¸ì¥ ì‚¬ìš©
        if not relevant_sentences:
            relevant_sentences = [s.strip() for s in sentences[:3] if s.strip()]

        # ìš”ì•½ ìƒì„±
        summary = '. '.join(relevant_sentences[:3])
        if summary and not summary.endswith('.'):
            summary += '.'

        return summary or content[:500]

    def chat(self):
        """ì¸í„°ë™í‹°ë¸Œ ì±„íŒ… ëª¨ë“œ"""
        print("="*70)
        print("ğŸ’¬ Web-Enhanced SimpleLLM ì±—ë´‡")
        print("="*70)
        print()
        print("ê¸°ëŠ¥:")
        print("  1. ì¼ë°˜ í…ìŠ¤íŠ¸ ì…ë ¥ â†’ ëª¨ë¸ì´ ì´ì–´ì„œ ìƒì„±")
        print("  2. ì§ˆë¬¸ ì…ë ¥ â†’ ëª¨ë¸ì´ ë‹µë³€ ìƒì„±")
        print("  3. ë‹µë³€ì´ ë¶€ì¡±í•˜ë©´ â†’ ìë™ìœ¼ë¡œ ì›¹ ê²€ìƒ‰")
        print()
        print("ëª…ë ¹ì–´:")
        print("  /search <ì§ˆë¬¸> - ì›¹ì—ì„œ ì§ì ‘ ê²€ìƒ‰")
        print("  /knowledge - ì €ì¥ëœ ì§€ì‹ í™•ì¸")
        print("  /help - ë„ì›€ë§")
        print("  quit/exit - ì¢…ë£Œ")
        print()
        print("-"*70)

        while True:
            try:
                user_input = input("\nğŸ“ ì…ë ¥: ").strip()

                if not user_input:
                    print("âš ï¸  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue

                # ì¢…ë£Œ ëª…ë ¹
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                # /search ëª…ë ¹ (ì§ì ‘ ì›¹ ê²€ìƒ‰)
                if user_input.startswith('/search '):
                    query = user_input[8:].strip()
                    if query:
                        answer = self.answer_with_web(query)
                        print(f"\nğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n{answer}")
                        print("-"*70)
                    else:
                        print("âš ï¸  ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: /search What is AI")
                    continue

                # /knowledge ëª…ë ¹ (ì§€ì‹ ë² ì´ìŠ¤ í™•ì¸)
                if user_input.startswith('/knowledge'):
                    if self.knowledge_base:
                        print("\nğŸ“š ì €ì¥ëœ ì§€ì‹:")
                        for i, (question, data) in enumerate(self.knowledge_base.items(), 1):
                            print(f"\n{i}. {question}")
                            print(f"   ì‹œê°„: {data['timestamp']}")
                            print(f"   ë‚´ìš©: {data['content'][:100]}...")
                    else:
                        print("\nâš ï¸  ì €ì¥ëœ ì§€ì‹ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # /help ëª…ë ¹
                if user_input.startswith('/help'):
                    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹:")
                    print("  /search <ì§ˆë¬¸> - ì›¹ì—ì„œ ì§ì ‘ ê²€ìƒ‰")
                    print("  /knowledge - ì €ì¥ëœ ì§€ì‹ í™•ì¸")
                    print("  /help - ë„ì›€ë§ í‘œì‹œ")
                    continue

                # ì§ˆë¬¸ì¸ì§€ í™•ì¸
                is_question_input = self.is_question(user_input)

                if is_question_input:
                    print("\nğŸ¤” ì§ˆë¬¸ìœ¼ë¡œ ì¸ì‹í–ˆìŠµë‹ˆë‹¤.")

                    # ë¨¼ì € ëª¨ë¸ë¡œ ë‹µë³€ ì‹œë„
                    if self.trained:
                        print("ğŸ¤– ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„± ì¤‘...")
                        model_response = self.generate_response(user_input, max_tokens=50)

                        if model_response and self.is_response_valid(user_input, model_response):
                            print(f"\nğŸ¤– ëª¨ë¸ ë‹µë³€:\n{model_response}")

                            # ì‚¬ìš©ìì—ê²Œ ë§Œì¡±ë„ í™•ì¸
                            satisfied = input("\nì´ ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ì‹ ê°€ìš”? (y/n): ").strip().lower()

                            if satisfied == 'y':
                                print("-"*70)
                                continue
                            else:
                                print("\në” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•´ ì›¹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")

                    # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë‹µë³€
                    web_answer = self.answer_with_web(user_input)
                    print(f"\nğŸŒ ì›¹ ê²€ìƒ‰ ë‹µë³€:\n{web_answer}")
                    print("-"*70)

                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
                    if self.trained:
                        print("\nğŸ¤– ìƒì„± ì¤‘...", end="", flush=True)
                        generated = self.generate_response(user_input, max_tokens=50)
                        print("\r" + " "*50 + "\r", end="")

                        if generated:
                            print(f"ğŸ¤– ì‘ë‹µ:\n{generated}")
                        else:
                            print("âš ï¸  ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        print("âš ï¸  ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /search ëª…ë ¹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

                    print("-"*70)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*70)
    print("ğŸ¤– Web-Enhanced SimpleLLM")
    print("="*70)
    print()

    checkpoint_path = 'checkpoints/best_model.pt'

    # ì±—ë´‡ ì‹¤í–‰
    chatbot = WebEnhancedChatbot(checkpoint_path)
    chatbot.chat()


if __name__ == "__main__":
    main()
