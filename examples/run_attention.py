import torch
import torch.nn as nn
import tiktoken
import sys


class TokenEmbedding(nn.Module):
    """í† í° IDë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""

    def __init__(self, vocab_size, d_embed):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_embed)

    def forward(self, token_ids):
        return self.embedding(token_ids)


class CausalAttention(nn.Module):
    """ì¸ê³¼ì  Self-Attention (GPT ìŠ¤íƒ€ì¼)"""

    def __init__(self, d_in, d_out):
        super().__init__()
        self.d_out = d_out

        self.W_query = nn.Linear(d_in, d_out, bias=False)
        self.W_key = nn.Linear(d_in, d_out, bias=False)
        self.W_value = nn.Linear(d_in, d_out, bias=False)

    def forward(self, x):
        batch_size, seq_len, d_in = x.shape

        queries = self.W_query(x)
        keys = self.W_key(x)
        values = self.W_value(x)

        # Attention scores ê³„ì‚°
        attn_scores = queries @ keys.transpose(1, 2)
        attn_scores_scaled = attn_scores / (self.d_out ** 0.5)

        # Causal mask ì ìš©
        mask = torch.tril(torch.ones(seq_len, seq_len))
        attn_scores_masked = attn_scores_scaled.masked_fill(mask == 0, float('-inf'))

        # Softmaxë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attn_weights = torch.softmax(attn_scores_masked, dim=-1)

        # Context vector ê³„ì‚°
        context = attn_weights @ values

        return context, attn_weights


class AttentionModel:
    """ì–´í…ì…˜ ëª¨ë¸"""

    def __init__(self, d_embed=128, d_out=128):
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.vocab_size = self.tokenizer.n_vocab

        self.embedding = TokenEmbedding(self.vocab_size, d_embed)
        self.attention = CausalAttention(d_embed, d_out)

    def process(self, text):
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        # í† í°í™”
        token_ids = self.tokenizer.encode(text)
        tokens = [self.tokenizer.decode([tid]) for tid in token_ids]

        # í…ì„œë¡œ ë³€í™˜
        token_tensor = torch.tensor(token_ids).unsqueeze(0)

        # ì„ë² ë”© + ì–´í…ì…˜
        embeddings = self.embedding(token_tensor)
        output, attn_weights = self.attention(embeddings)

        return output, tokens, attn_weights

    def visualize(self, tokens, attn_weights):
        """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        print(f"\n{'='*80}")
        print("ğŸ” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
        print(f"{'='*80}")
        print("ê° í† í°(í–‰)ì´ ì´ì „ í† í°ë“¤(ì—´)ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")

        attn_matrix = attn_weights[0].detach().numpy()

        # í—¤ë”
        max_len = max(len(t) for t in tokens)
        header = " " * (max_len + 2)
        for token in tokens:
            header += f"{token[:8]:>10}"
        print(header)
        print("-" * len(header))

        # ê° í–‰
        for i, token in enumerate(tokens):
            row = f"{token[:max_len]:<{max_len+2}}"
            for j in range(len(tokens)):
                if j <= i:
                    row += f"{attn_matrix[i, j]:>10.3f}"
                else:
                    row += f"{'---':>10}"
            print(row)
        print()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*80)
    print("ğŸ¤– ì–´í…ì…˜ ëª¨ë¸ - í…ìŠ¤íŠ¸ ì…ë ¥ ë°›ê¸°")
    print("="*80)
    print()

    # ëª¨ë¸ ì´ˆê¸°í™”
    torch.manual_seed(42)
    model = AttentionModel(d_embed=128, d_out=128)
    print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\n")

    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìë¡œ ì…ë ¥ ë°›ê¸°
    if len(sys.argv) > 1:
        # ëª…ë ¹ì–´ë¡œ ì‹¤í–‰: python run_attention.py "your text here"
        text = " ".join(sys.argv[1:])
        print(f"ğŸ“ ì…ë ¥: \"{text}\"\n")

        # ì²˜ë¦¬
        output, tokens, attn_weights = model.process(text)

        print(f"í† í° ê°œìˆ˜: {len(tokens)}")
        print(f"í† í°: {tokens}")
        print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")

        # ì‹œê°í™”
        model.visualize(tokens, attn_weights)

    else:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        print("ğŸ’¡ ì‚¬ìš©ë²•: python run_attention.py \"your text here\"")
        print("ë˜ëŠ” ì•„ë˜ì— ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”.\n")

        while True:
            try:
                text = input("ğŸ“ í…ìŠ¤íŠ¸ ì…ë ¥ (ì¢…ë£Œ: Ctrl+C): ").strip()

                if not text:
                    print("âš ï¸  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                # ì²˜ë¦¬
                output, tokens, attn_weights = model.process(text)

                print(f"\ní† í° ê°œìˆ˜: {len(tokens)}")
                print(f"í† í°: {tokens}")
                print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")

                # ì‹œê°í™”
                model.visualize(tokens, attn_weights)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}\n")


if __name__ == "__main__":
    main()
