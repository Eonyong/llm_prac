"""
LLM í•™ìŠµ íŒŒì´í”„ë¼ì¸

í•™ìŠµì— í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ:
1. ì†ì‹¤ í•¨ìˆ˜ (Loss Function): Cross Entropy Loss
2. ì˜µí‹°ë§ˆì´ì € (Optimizer): AdamW
3. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Learning Rate Scheduler): Warmup + Cosine Decay
4. ë°ì´í„°ë¡œë” (DataLoader): ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ì œê³µ
5. í•™ìŠµ ë£¨í”„ (Training Loop): ì‹¤ì œ í•™ìŠµ ì§„í–‰
6. ê²€ì¦ (Validation): ê³¼ì í•© ë°©ì§€
7. ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Checkpointing): ëª¨ë¸ ì €ì¥/ë¡œë“œ
8. í‰ê°€ ì§€í‘œ (Metrics): Perplexity, Accuracy
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import tiktoken
import math
import os
from tqdm import tqdm
import time

from layer_normalization import SimpleLLM


# ===================================================================
# 1. ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ===================================================================

class TextDataset(Dataset):
    """
    ì–¸ì–´ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹

    ì…ë ¥: [í† í°1, í† í°2, í† í°3, í† í°4]
    íƒ€ê²Ÿ: [í† í°2, í† í°3, í† í°4, í† í°5]
    â†’ ë‹¤ìŒ í† í° ì˜ˆì¸¡ í•™ìŠµ
    """

    def __init__(self, text_path, tokenizer, max_length=256, stride=None):
        """
        Args:
            text_path: í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            tokenizer: í† í¬ë‚˜ì´ì €
            max_length: ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´
            stride: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê°„ê²© (Noneì´ë©´ max_lengthì™€ ë™ì¼)
        """
        # í…ìŠ¤íŠ¸ ë¡œë“œ
        with open(text_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # í† í°í™”
        self.token_ids = tokenizer.encode(text)
        self.max_length = max_length
        self.stride = stride if stride is not None else max_length

        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        print(f"   - ì´ ë¬¸ì ìˆ˜: {len(text):,}")
        print(f"   - ì´ í† í° ìˆ˜: {len(self.token_ids):,}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {max_length}")
        print(f"   - Stride: {self.stride}")

    def __len__(self):
        # ìƒì„± ê°€ëŠ¥í•œ ìƒ˜í”Œ ìˆ˜
        return (len(self.token_ids) - self.max_length - 1) // self.stride

    def __getitem__(self, idx):
        # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
        start_idx = idx * self.stride

        # ì…ë ¥ê³¼ íƒ€ê²Ÿ ì¶”ì¶œ
        input_ids = self.token_ids[start_idx:start_idx + self.max_length]
        target_ids = self.token_ids[start_idx + 1:start_idx + self.max_length + 1]

        return torch.tensor(input_ids), torch.tensor(target_ids)


# ===================================================================
# 2. ì†ì‹¤ í•¨ìˆ˜ ë° í‰ê°€ ì§€í‘œ
# ===================================================================

def compute_loss(logits, targets):
    """
    Cross Entropy Loss ê³„ì‚°

    Args:
        logits: [batch_size, seq_len, vocab_size]
        targets: [batch_size, seq_len]

    Returns:
        loss: ìŠ¤ì¹¼ë¼ ì†ì‹¤ ê°’
    """
    # logitsë¥¼ 2Dë¡œ, targetsë¥¼ 1Dë¡œ ë³€í™˜
    batch_size, seq_len, vocab_size = logits.shape
    logits_flat = logits.view(-1, vocab_size)  # [batch_size * seq_len, vocab_size]
    targets_flat = targets.view(-1)  # [batch_size * seq_len]

    # Cross Entropy Loss
    loss = F.cross_entropy(logits_flat, targets_flat)

    return loss


def compute_perplexity(loss):
    """
    Perplexity ê³„ì‚°

    Perplexity = exp(loss)
    ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ëª¨ë¸ì´ ë‹¤ìŒ í† í°ì„ ì˜ ì˜ˆì¸¡í•¨)
    """
    return torch.exp(loss)


def compute_accuracy(logits, targets):
    """
    ì •í™•ë„ ê³„ì‚°

    Args:
        logits: [batch_size, seq_len, vocab_size]
        targets: [batch_size, seq_len]

    Returns:
        accuracy: ì •í™•ë„ (0~1)
    """
    predictions = torch.argmax(logits, dim=-1)  # [batch_size, seq_len]
    correct = (predictions == targets).float()
    accuracy = correct.mean()
    return accuracy


# ===================================================================
# 3. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Warmup + Cosine Decay)
# ===================================================================

class WarmupCosineScheduler:
    """
    Warmup + Cosine Decay í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬

    1. Warmup ë‹¨ê³„: í•™ìŠµë¥ ì„ 0ì—ì„œ max_lrê¹Œì§€ ì„ í˜• ì¦ê°€
    2. Cosine Decay: í•™ìŠµë¥ ì„ ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ê°ì†Œ
    """

    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=1e-6):
        """
        Args:
            optimizer: ì˜µí‹°ë§ˆì´ì €
            warmup_steps: Warmup ìŠ¤í… ìˆ˜
            total_steps: ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜
            min_lr: ìµœì†Œ í•™ìŠµë¥ 
        """
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']
        self.current_step = 0

    def step(self):
        """í•™ìŠµë¥  ì—…ë°ì´íŠ¸"""
        self.current_step += 1

        if self.current_step < self.warmup_steps:
            # Warmup ë‹¨ê³„: ì„ í˜• ì¦ê°€
            lr = self.base_lr * (self.current_step / self.warmup_steps)
        else:
            # Cosine Decay ë‹¨ê³„
            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))

        # ì˜µí‹°ë§ˆì´ì €ì— ì ìš©
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        return lr


# ===================================================================
# 4. í•™ìŠµ í´ë˜ìŠ¤
# ===================================================================

class Trainer:
    """
    LLM í•™ìŠµ ë‹´ë‹¹ í´ë˜ìŠ¤
    """

    def __init__(self, model, train_loader, val_loader, device, config):
        """
        Args:
            model: SimpleLLM ëª¨ë¸
            train_loader: í•™ìŠµ ë°ì´í„°ë¡œë”
            val_loader: ê²€ì¦ ë°ì´í„°ë¡œë”
            device: ë””ë°”ì´ìŠ¤ (cuda/mps/cpu)
            config: í•™ìŠµ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.config = config

        # ì˜µí‹°ë§ˆì´ì € (AdamW)
        self.optimizer = AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            betas=(0.9, 0.95)  # GPT-3 ì„¤ì •
        )

        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        total_steps = len(train_loader) * config['num_epochs']
        warmup_steps = int(total_steps * config['warmup_ratio'])
        self.scheduler = WarmupCosineScheduler(
            self.optimizer,
            warmup_steps=warmup_steps,
            total_steps=total_steps,
            min_lr=config['min_lr']
        )

        # í•™ìŠµ ê¸°ë¡
        self.history = {
            'train_loss': [],
            'train_perplexity': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_perplexity': [],
            'val_accuracy': [],
            'lr': []
        }

        self.best_val_loss = float('inf')

    def train_epoch(self, epoch):
        """1 ì—í¬í¬ í•™ìŠµ"""
        self.model.train()

        total_loss = 0
        total_accuracy = 0
        num_batches = 0

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}")

        for inputs, targets in pbar:
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)

            # ìˆœì „íŒŒ
            logits = self.model(inputs)

            # ì†ì‹¤ ê³„ì‚°
            loss = compute_loss(logits, targets)

            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()

            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ê¸°ìš¸ê¸° í­ë°œ ë°©ì§€)
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config['grad_clip']
            )

            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            self.optimizer.step()

            # í•™ìŠµë¥  ì—…ë°ì´íŠ¸
            current_lr = self.scheduler.step()

            # ì •í™•ë„ ê³„ì‚°
            accuracy = compute_accuracy(logits, targets)

            # ê¸°ë¡
            total_loss += loss.item()
            total_accuracy += accuracy.item()
            num_batches += 1

            # ì§„í–‰ í‘œì‹œ
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'acc': f"{accuracy.item():.4f}",
                'lr': f"{current_lr:.2e}"
            })

        # í‰ê·  ê³„ì‚°
        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches
        avg_perplexity = math.exp(avg_loss)

        return avg_loss, avg_perplexity, avg_accuracy, current_lr

    def validate(self):
        """ê²€ì¦"""
        self.model.eval()

        total_loss = 0
        total_accuracy = 0
        num_batches = 0

        with torch.no_grad():
            for inputs, targets in tqdm(self.val_loader, desc="Validation"):
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)

                # ìˆœì „íŒŒ
                logits = self.model(inputs)

                # ì†ì‹¤ ê³„ì‚°
                loss = compute_loss(logits, targets)
                accuracy = compute_accuracy(logits, targets)

                total_loss += loss.item()
                total_accuracy += accuracy.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches
        avg_perplexity = math.exp(avg_loss)

        return avg_loss, avg_perplexity, avg_accuracy

    def train(self):
        """ì „ì²´ í•™ìŠµ ì‹¤í–‰"""
        print("\n" + "="*80)
        print("ğŸš€ í•™ìŠµ ì‹œì‘")
        print("="*80)

        start_time = time.time()

        for epoch in range(self.config['num_epochs']):
            # í•™ìŠµ
            train_loss, train_ppl, train_acc, lr = self.train_epoch(epoch)

            # ê²€ì¦
            val_loss, val_ppl, val_acc = self.validate()

            # ê¸°ë¡
            self.history['train_loss'].append(train_loss)
            self.history['train_perplexity'].append(train_ppl)
            self.history['train_accuracy'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_perplexity'].append(val_ppl)
            self.history['val_accuracy'].append(val_acc)
            self.history['lr'].append(lr)

            # ì¶œë ¥
            print(f"\nEpoch {epoch+1}/{self.config['num_epochs']}")
            print(f"  Train Loss: {train_loss:.4f}, PPL: {train_ppl:.2f}, Acc: {train_acc:.4f}")
            print(f"  Val   Loss: {val_loss:.4f}, PPL: {val_ppl:.2f}, Acc: {val_acc:.4f}")
            print(f"  LR: {lr:.2e}")

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint(epoch, is_best=True)
                print(f"  âœ… ìµœê³  ëª¨ë¸ ì €ì¥! (Val Loss: {val_loss:.4f})")

            # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸
            if (epoch + 1) % self.config['save_every'] == 0:
                self.save_checkpoint(epoch, is_best=False)

        elapsed_time = time.time() - start_time
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed_time/60:.2f}ë¶„")

    def save_checkpoint(self, epoch, is_best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        os.makedirs(self.config['checkpoint_dir'], exist_ok=True)

        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'history': self.history,
            'config': self.config
        }

        if is_best:
            path = os.path.join(self.config['checkpoint_dir'], 'best_model.pt')
        else:
            path = os.path.join(self.config['checkpoint_dir'], f'checkpoint_epoch_{epoch+1}.pt')

        torch.save(checkpoint, path)

    def load_checkpoint(self, path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.best_val_loss = checkpoint['best_val_loss']
        self.history = checkpoint['history']
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {path}")
        return checkpoint['epoch']


# ===================================================================
# 5. ë©”ì¸ ì‹¤í–‰
# ===================================================================

def main():
    print("="*80)
    print("ğŸ¤– SimpleLLM í•™ìŠµ íŒŒì´í”„ë¼ì¸")
    print("="*80)
    print()

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print("âœ… GPU ì‚¬ìš© (CUDA)")
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("âœ… GPU ì‚¬ìš© (MPS - Apple Silicon)")
    else:
        device = torch.device('cpu')
        print("âš ï¸  CPU ì‚¬ìš© (í•™ìŠµì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()

    # í•™ìŠµ ì„¤ì •
    config = {
        # ëª¨ë¸ ì„¤ì •
        'd_embed': 256,
        'num_heads': 4,
        'num_layers': 4,
        'max_seq_len': 256,
        'dropout': 0.1,

        # í•™ìŠµ ì„¤ì •
        'batch_size': 8,
        'num_epochs': 10,
        'learning_rate': 3e-4,
        'min_lr': 3e-5,
        'weight_decay': 0.1,
        'warmup_ratio': 0.1,
        'grad_clip': 1.0,

        # ë°ì´í„° ì„¤ì •
        'train_stride': 128,  # ì˜¤ë²„ë© í—ˆìš©
        'val_stride': 256,    # ê²€ì¦ì€ ì˜¤ë²„ë© ì—†ì´

        # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
        'checkpoint_dir': './checkpoints',
        'save_every': 2  # 2 ì—í¬í¬ë§ˆë‹¤ ì €ì¥
    }

    # í† í¬ë‚˜ì´ì €
    tokenizer = tiktoken.get_encoding("gpt2")
    vocab_size = tokenizer.n_vocab

    # ë°ì´í„°ì…‹ ìƒì„±
    print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    try:
        train_dataset = TextDataset(
            'the-verdict.txt',
            tokenizer,
            max_length=config['max_seq_len'],
            stride=config['train_stride']
        )

        # ê²€ì¦ ë°ì´í„°ì…‹ (ë§ˆì§€ë§‰ 10% ì‚¬ìš©)
        train_size = int(0.9 * len(train_dataset))
        val_size = len(train_dataset) - train_size

        train_dataset, val_dataset = torch.utils.data.random_split(
            train_dataset, [train_size, val_size]
        )

        print(f"   - í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_dataset):,}")
        print(f"   - ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(val_dataset):,}\n")

    except FileNotFoundError:
        print("âŒ 'the-verdict.txt' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.\n")
        return

    # ë°ì´í„°ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=0,  # Windowsì—ì„œëŠ” 0ìœ¼ë¡œ ì„¤ì •
        pin_memory=True if device.type == 'cuda' else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )

    # ëª¨ë¸ ìƒì„±
    print("ğŸ—ï¸  ëª¨ë¸ ìƒì„± ì¤‘...")
    model = SimpleLLM(
        vocab_size=vocab_size,
        d_embed=config['d_embed'],
        num_heads=config['num_heads'],
        num_layers=config['num_layers'],
        max_seq_len=config['max_seq_len'],
        dropout=config['dropout']
    )

    # í•™ìŠµ
    trainer = Trainer(model, train_loader, val_loader, device, config)
    trainer.train()

    print("\n" + "="*80)
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print("="*80)
    print(f"ìµœê³  ê²€ì¦ ì†ì‹¤: {trainer.best_val_loss:.4f}")
    print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {config['checkpoint_dir']}")
    print()


if __name__ == "__main__":
    main()
