# SimpleLLM - ê°„ë‹¨í•œ ì–¸ì–´ ëª¨ë¸ í”„ë¡œì íŠ¸

PyTorchë¥¼ ì‚¬ìš©í•œ GPT ìŠ¤íƒ€ì¼ì˜ ì–¸ì–´ ëª¨ë¸ êµ¬í˜„ ë° í•™ìŠµ

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
LLM/
â”œâ”€â”€ layer_normalization.py      # í•µì‹¬ ëª¨ë¸ ì½”ë“œ (SimpleLLM, Transformer ë¸”ë¡)
â”œâ”€â”€ training_pipeline.py         # í•™ìŠµ íŒŒì´í”„ë¼ì¸ (ë°ì´í„°ë¡œë”, í•™ìŠµ ë£¨í”„, ì˜µí‹°ë§ˆì´ì €)
â”œâ”€â”€ run_simple_llm.py           # í…ìŠ¤íŠ¸ ìƒì„± ë° ë¶„ì„ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ training_guide.md           # í•™ìŠµ ê°€ì´ë“œ ë¬¸ì„œ
â”œâ”€â”€ the-verdict.txt             # í•™ìŠµ ë°ì´í„°
â”œâ”€â”€ checkpoints/                # í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜
â””â”€â”€ examples/                   # ì˜ˆì œ ë° ë°ëª¨ ì½”ë“œ
    â”œâ”€â”€ chapter3.py
    â”œâ”€â”€ main.py
    â”œâ”€â”€ simple_attention.py
    â”œâ”€â”€ multihead_attention.py
    â”œâ”€â”€ interactive_attention.py
    â”œâ”€â”€ run_attention.py
    â””â”€â”€ ...
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source .venv/bin/activate

# í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
pip list | grep -E "torch|tiktoken|tqdm"
```

### 2. ë©”ì¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰ (ì¶”ì²œ)

```bash
# main.py ì‹¤í–‰ - ìë™ìœ¼ë¡œ í•™ìŠµ ë˜ëŠ” ì±—ë´‡ ì‹¤í–‰
python main.py
```

í”„ë¡œê·¸ë¨ì´ ìë™ìœ¼ë¡œ:
- í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ â†’ í•™ìŠµ ì§„í–‰
- í•™ìŠµëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ â†’ ì±—ë´‡ ì‹œì‘

### 3. ë˜ëŠ” ìˆ˜ë™ í•™ìŠµ

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ ì‹œì‘
python training_pipeline.py
```

**í•™ìŠµ ì„¤ì • ìˆ˜ì •**: `training_pipeline.py`ì˜ `config` ë”•ì…”ë„ˆë¦¬ ìˆ˜ì •

```python
config = {
    'd_embed': 256,        # ì„ë² ë”© ì°¨ì›
    'num_heads': 4,        # í—¤ë“œ ê°œìˆ˜
    'num_layers': 4,       # ë ˆì´ì–´ ê°œìˆ˜
    'batch_size': 8,       # ë°°ì¹˜ í¬ê¸°
    'num_epochs': 10,      # ì—í¬í¬ ìˆ˜
    'learning_rate': 3e-4, # í•™ìŠµë¥ 
}
```

### 4. ì±—ë´‡ ì‚¬ìš© (main.py)

```bash
# ì¸í„°ë™í‹°ë¸Œ ì±—ë´‡ ì‹œì‘
python main.py
```

**ì±—ë´‡ ëª…ë ¹ì–´:**
- ì¼ë°˜ í…ìŠ¤íŠ¸ ì…ë ¥ â†’ ì´ì–´ì„œ ìƒì„±
- `/temp 0.8` â†’ Temperature ì„¤ì • (ë‹¤ì–‘ì„± ì¡°ì ˆ)
- `/tokens 100` â†’ ìƒì„±í•  í† í° ìˆ˜ ì„¤ì •
- `/topk 30` â†’ Top-k ìƒ˜í”Œë§ ì„¤ì •
- `/help` â†’ ë„ì›€ë§ í‘œì‹œ
- `quit` ë˜ëŠ” `exit` â†’ ì¢…ë£Œ

**ì˜ˆì‹œ:**
```
ğŸ“ ì…ë ¥: Once upon a time
ğŸ¤– ì‘ë‹µ: Once upon a time, there was a small village...

ğŸ“ ì…ë ¥: /temp 1.2
âœ… Temperature ì„¤ì •: 1.2

ğŸ“ ì…ë ¥: The weather is
ğŸ¤– ì‘ë‹µ: The weather is beautiful today, with clear skies...
```

### 5. ì¶”ê°€ ë„êµ¬ (run_simple_llm.py)

```bash
# ë‹¤ìŒ í† í° ì˜ˆì¸¡ ë¶„ì„
python run_simple_llm.py analyze "Hello, how are you"

# í…ìŠ¤íŠ¸ ìƒì„±
python run_simple_llm.py generate "Once upon a time"

# ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
python run_simple_llm.py
```

---

## ğŸ“š í•µì‹¬ íŒŒì¼ ì„¤ëª…

### `layer_normalization.py`
**ëª¨ë¸ ì•„í‚¤í…ì²˜ ì½”ë“œ**

í¬í•¨ëœ í´ë˜ìŠ¤:
- `LayerNorm`: ì¸µ ì •ê·œí™”
- `FeedForward`: í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
- `MultiHeadAttention`: ë©€í‹°-í—¤ë“œ ì–´í…ì…˜
- `TransformerBlock`: Transformer ë¸”ë¡ (Pre-LN ìŠ¤íƒ€ì¼)
- `SimpleLLM`: ì™„ì „í•œ ì–¸ì–´ ëª¨ë¸ (GPT ìŠ¤íƒ€ì¼)

### `training_pipeline.py`
**í•™ìŠµ íŒŒì´í”„ë¼ì¸**

í¬í•¨ëœ í´ë˜ìŠ¤:
- `TextDataset`: í•™ìŠµ ë°ì´í„°ì…‹
- `WarmupCosineScheduler`: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
- `Trainer`: í•™ìŠµ ë£¨í”„ ë° ê²€ì¦

ì£¼ìš” ê¸°ëŠ¥:
- Cross Entropy Loss
- AdamW ì˜µí‹°ë§ˆì´ì €
- Gradient Clipping
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ
- Perplexity & Accuracy ê³„ì‚°

### `run_simple_llm.py`
**í…ìŠ¤íŠ¸ ìƒì„± ë° ë¶„ì„**

ì£¼ìš” ê¸°ëŠ¥:
- ë‹¤ìŒ í† í° ì˜ˆì¸¡ (Top-k)
- í…ìŠ¤íŠ¸ ìë™ ìƒì„±
- ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ

---

## ğŸ¯ ëª¨ë¸ êµ¬ì¡°

```
SimpleLLM Architecture:

Input Tokens
    â†“
Token Embedding + Positional Embedding
    â†“
Dropout
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block Ã— N          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ LayerNorm                 â”‚  â”‚
â”‚  â”‚    â†“                      â”‚  â”‚
â”‚  â”‚ Multi-Head Attention      â”‚  â”‚
â”‚  â”‚    â†“                      â”‚  â”‚
â”‚  â”‚ Residual Connection       â”‚  â”‚
â”‚  â”‚    â†“                      â”‚  â”‚
â”‚  â”‚ LayerNorm                 â”‚  â”‚
â”‚  â”‚    â†“                      â”‚  â”‚
â”‚  â”‚ Feed Forward Network      â”‚  â”‚
â”‚  â”‚    â†“                      â”‚  â”‚
â”‚  â”‚ Residual Connection       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final LayerNorm
    â†“
Output Projection (Logits)
```

---

## ğŸ“Š í•™ìŠµ ì˜ˆì œ

### ê¸°ë³¸ í•™ìŠµ ê²°ê³¼ (the-verdict.txt)

```
ë°ì´í„°ì…‹: 20,479 ë¬¸ì, 5,145 í† í°
í•™ìŠµ ì‹œê°„: ~30ì´ˆ (Apple Silicon GPU)

Epoch 1:  PPL: 41,729 â†’ Acc: 0.59%
Epoch 5:  PPL: 5,740  â†’ Acc: 3.61%
Epoch 10: PPL: 2,874  â†’ Acc: 3.61%
```

---

## ğŸ”§ í•™ìŠµ íŒ

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```python
config = {
    'batch_size': 4,      # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
    'max_seq_len': 128,   # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
    'd_embed': 128,       # ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
}
```

### 2. í•™ìŠµ ì†ë„ í–¥ìƒ
```python
# ë” í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš© (ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ)
config['batch_size'] = 16

# ë” ì§§ì€ ì‹œí€€ìŠ¤ ê¸¸ì´
config['max_seq_len'] = 128
```

### 3. ì„±ëŠ¥ í–¥ìƒ
```python
# ë” í° ëª¨ë¸
config = {
    'd_embed': 512,
    'num_layers': 6,
    'num_heads': 8,
}

# ë” ë§ì€ ì—í¬í¬
config['num_epochs'] = 20
```

---

## ğŸ“– ì¶”ê°€ ìë£Œ

- `training_guide.md`: ìƒì„¸í•œ í•™ìŠµ ê°€ì´ë“œ
- `examples/`: ë‹¤ì–‘í•œ ì˜ˆì œ ì½”ë“œ
  - `simple_attention.py`: ê¸°ë³¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
  - `multihead_attention.py`: ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ë°ëª¨
  - `interactive_attention.py`: ì¸í„°ë™í‹°ë¸Œ ì–´í…ì…˜ ì‹œê°í™”

---

## ğŸ“ í•™ìŠµ ë‚´ìš©

ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê²ƒë“¤:

1. **Transformer ì•„í‚¤í…ì²˜**
   - Multi-Head Attention
   - Layer Normalization
   - Residual Connections
   - Feed Forward Networks

2. **í•™ìŠµ ê¸°ë²•**
   - AdamW ì˜µí‹°ë§ˆì´ì €
   - Warmup + Cosine Decay ìŠ¤ì¼€ì¤„ëŸ¬
   - Gradient Clipping
   - Dropout (ì •ê·œí™”)

3. **LLM ê¸°ì´ˆ**
   - Token Embedding
   - Positional Embedding
   - Causal Masking
   - Next Token Prediction

---

## ğŸ” ë¬¸ì œ í•´ê²°

### Q: í•™ìŠµì´ ë„ˆë¬´ ëŠë ¤ìš”
```
A: GPUë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
   ì¶œë ¥ì— "âœ… GPU ì‚¬ìš© (MPS)" ë˜ëŠ” "âœ… GPU ì‚¬ìš© (CUDA)"ê°€ í‘œì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
```

### Q: ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•Šì•„ìš”
```
A: 1. í•™ìŠµë¥ ì„ ë‚®ì¶”ì„¸ìš” (3e-4 â†’ 1e-4)
   2. ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•˜ì„¸ìš”
   3. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°’ í™•ì¸ (grad_clip = 1.0)
```

### Q: ëª¨ë¸ì´ ê°™ì€ ë‹¨ì–´ë§Œ ë°˜ë³µí•´ìš”
```
A: 1. ë” ë§ì€ ë°ì´í„°ë¡œ ë” ì˜¤ë˜ í•™ìŠµí•˜ì„¸ìš”
   2. Temperatureë¥¼ ì¡°ì •í•˜ì„¸ìš” (í…ìŠ¤íŠ¸ ìƒì„± ì‹œ)
   3. Top-k ë˜ëŠ” Top-p ìƒ˜í”Œë§ ì‚¬ìš©
```

---

## ğŸ“ TODO (í–¥í›„ ê°œì„  ì‚¬í•­)

- [ ] Gradient Accumulation êµ¬í˜„
- [ ] Mixed Precision Training ì¶”ê°€
- [ ] TensorBoard ë¡œê¹…
- [ ] Early Stopping
- [ ] Top-k, Top-p ìƒ˜í”Œë§
- [ ] Beam Search
- [ ] Model Export (ONNX)
- [ ] ë” í° ë°ì´í„°ì…‹ ì˜ˆì œ

---

## ğŸ“œ ë¼ì´ì„¼ìŠ¤

MIT License

---

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆì™€ Pull Requestë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

---

## ğŸ“§ ì—°ë½ì²˜

í”„ë¡œì íŠ¸ ë¬¸ì˜: [ì´ë©”ì¼ ì£¼ì†Œ]

---

**Happy Learning! ğŸš€**
